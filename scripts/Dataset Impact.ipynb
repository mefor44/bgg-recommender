{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43f8a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "import scipy.sparse\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from metrics import Evaluator\n",
    "from utils_VAE import BaseMultiVAE, TrainableMultVAE, loss_function, naive_sparse2tensor, sparse2torch_sparse\n",
    "from my_models import ParallSynSLIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22e54ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 1362961 rows with score <= 6.\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore',SparseEfficiencyWarning)\n",
    "\n",
    "mypath = \"/home/mmarzec12/data/\"\n",
    "savepath = \"/home/mmarzec12/models/vae/model_tuning/\"\n",
    "\n",
    "explicit = pd.read_csv(mypath+\"explicit_train.csv\")\n",
    "validation = pd.read_csv(mypath+\"leave_one_out_validation.csv\")\n",
    "test = pd.read_csv(mypath+\"leave_one_out_test.csv\")\n",
    "\n",
    "# list with (user,item) tuples from validation set\n",
    "test_list = [(u,i) for u,i in zip(test.user_name, test.game_id)]\n",
    "# dict with user:game key-value pairs from validation set\n",
    "test_dict = {u:i for u,i in zip(test.user_name, test.game_id)}\n",
    "\n",
    "# unique games and users\n",
    "unique_users = explicit.user_name.unique()\n",
    "unique_games = explicit.game_id.unique()\n",
    "\n",
    "n_users, n_items = len(unique_users), len(unique_games)\n",
    "\n",
    "# dictonaries to map users to unique ids and vice vers\n",
    "us_to_ids = {u:i for i,u in enumerate(unique_users)}\n",
    "ids_to_us = {i:u for i,u in enumerate(unique_users)}\n",
    "\n",
    "# dictonaries to map games to unique ids and vice vers\n",
    "gs_to_ids = {g:i for i,g in enumerate(unique_games)}\n",
    "ids_to_gs = {i:g for i,g in enumerate(unique_games)}\n",
    "\n",
    "\n",
    "implicit = pd.read_csv(mypath+\"implicit_train.csv\")\n",
    "implicit[\"score\"] = 1\n",
    "\n",
    "# filtering explicit ratings: filter ratings <6 and >=1\n",
    "print(f\"There is {np.sum(explicit.score <= 6)} rows with score <= 6.\")\n",
    "#explicit = explicit[explicit.score > 6]\n",
    "\n",
    "# we join implictit and explicit rating data\n",
    "joined = pd.concat([explicit, implicit])\n",
    "joined = joined[[\"user_name\", \"game_id\", \"score\"]]\n",
    "# converting all interaction data to \"1\" \n",
    "joined[\"score\"] = 1\n",
    "\n",
    "# creating sparse matrix with data\n",
    "row = [us_to_ids[us] for us in joined.user_name]\n",
    "col = [gs_to_ids[g] for g in joined.game_id]\n",
    "data = joined.score\n",
    "\n",
    "train_data = scipy.sparse.coo_matrix((data, (row, col)), shape=(len(unique_users), len(unique_games))).tocsr()\n",
    "#item_matrix = user_matrix.T.copy()\n",
    "#dok_matrix = user_matrix.todok()\n",
    "\n",
    "user_loc = row\n",
    "\n",
    "\n",
    "item_loc = col\n",
    "ratings = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1586305f",
   "metadata": {},
   "source": [
    "## Explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4d763",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81302f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_savepath = \"/home/mmarzec12/models/vae/dataset_impact/\"\n",
    "best_params = pd.read_pickle(\"/home/mmarzec12/models/vae/model_tuning/\"+\"vae_best_params\")\n",
    "base_params = best_params[\"base_params\"]\n",
    "train_params = best_params[\"train_params\"]\n",
    "criterion = loss_function\n",
    "\n",
    "# preparing test data\n",
    "test_data = [(us_to_ids[u], gs_to_ids[i]) for u,i in test_list]\n",
    "\n",
    "# Explicit dataset\n",
    "row = [us_to_ids[us] for us in explicit.user_name]\n",
    "col = [gs_to_ids[g] for g in explicit.game_id]\n",
    "data = explicit.score\n",
    "\n",
    "train_data = scipy.sparse.coo_matrix((data, (row, col)), shape=(len(unique_users), len(unique_games))).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b90a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e036caca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase...\n",
      "| epoch   1 |  100/ 427 batches | ms/batch 32.90 | loss 2823.57\n",
      "| epoch   1 |  200/ 427 batches | ms/batch 32.54 | loss 2642.88\n",
      "| epoch   1 |  300/ 427 batches | ms/batch 32.40 | loss 2638.54\n",
      "| epoch   1 |  400/ 427 batches | ms/batch 32.40 | loss 2563.20\n",
      "Training took 13.84 seconds.\n",
      "Training phase...\n",
      "| epoch   2 |  100/ 427 batches | ms/batch 32.30 | loss 2555.63\n",
      "| epoch   2 |  200/ 427 batches | ms/batch 32.12 | loss 2551.39\n",
      "| epoch   2 |  300/ 427 batches | ms/batch 32.19 | loss 2538.95\n",
      "| epoch   2 |  400/ 427 batches | ms/batch 32.20 | loss 2512.93\n",
      "Training took 13.7 seconds.\n",
      "Training phase...\n",
      "| epoch   3 |  100/ 427 batches | ms/batch 32.71 | loss 2531.43\n",
      "| epoch   3 |  200/ 427 batches | ms/batch 32.19 | loss 2469.92\n",
      "| epoch   3 |  300/ 427 batches | ms/batch 32.21 | loss 2480.61\n",
      "| epoch   3 |  400/ 427 batches | ms/batch 32.40 | loss 2494.99\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch   4 |  100/ 427 batches | ms/batch 32.46 | loss 2485.54\n",
      "| epoch   4 |  200/ 427 batches | ms/batch 32.38 | loss 2493.96\n",
      "| epoch   4 |  300/ 427 batches | ms/batch 32.25 | loss 2437.73\n",
      "| epoch   4 |  400/ 427 batches | ms/batch 32.03 | loss 2473.73\n",
      "Training took 13.73 seconds.\n",
      "Training phase...\n",
      "| epoch   5 |  100/ 427 batches | ms/batch 32.62 | loss 2496.80\n",
      "| epoch   5 |  200/ 427 batches | ms/batch 32.19 | loss 2408.21\n",
      "| epoch   5 |  300/ 427 batches | ms/batch 32.23 | loss 2437.20\n",
      "| epoch   5 |  400/ 427 batches | ms/batch 32.45 | loss 2452.39\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch   6 |  100/ 427 batches | ms/batch 32.80 | loss 2465.67\n",
      "| epoch   6 |  200/ 427 batches | ms/batch 32.60 | loss 2398.85\n",
      "| epoch   6 |  300/ 427 batches | ms/batch 32.33 | loss 2448.72\n",
      "| epoch   6 |  400/ 427 batches | ms/batch 31.79 | loss 2437.86\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch   7 |  100/ 427 batches | ms/batch 32.37 | loss 2459.81\n",
      "| epoch   7 |  200/ 427 batches | ms/batch 32.41 | loss 2430.91\n",
      "| epoch   7 |  300/ 427 batches | ms/batch 32.68 | loss 2403.03\n",
      "| epoch   7 |  400/ 427 batches | ms/batch 32.00 | loss 2411.86\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch   8 |  100/ 427 batches | ms/batch 32.61 | loss 2452.90\n",
      "| epoch   8 |  200/ 427 batches | ms/batch 32.26 | loss 2414.34\n",
      "| epoch   8 |  300/ 427 batches | ms/batch 32.04 | loss 2436.01\n",
      "| epoch   8 |  400/ 427 batches | ms/batch 32.04 | loss 2386.21\n",
      "Training took 13.72 seconds.\n",
      "Training phase...\n",
      "| epoch   9 |  100/ 427 batches | ms/batch 32.25 | loss 2432.01\n",
      "| epoch   9 |  200/ 427 batches | ms/batch 31.92 | loss 2403.05\n",
      "| epoch   9 |  300/ 427 batches | ms/batch 32.38 | loss 2422.02\n",
      "| epoch   9 |  400/ 427 batches | ms/batch 32.27 | loss 2406.94\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  10 |  100/ 427 batches | ms/batch 32.69 | loss 2409.79\n",
      "| epoch  10 |  200/ 427 batches | ms/batch 32.35 | loss 2410.70\n",
      "| epoch  10 |  300/ 427 batches | ms/batch 32.16 | loss 2405.91\n",
      "| epoch  10 |  400/ 427 batches | ms/batch 32.27 | loss 2401.80\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  11 |  100/ 427 batches | ms/batch 32.59 | loss 2443.48\n",
      "| epoch  11 |  200/ 427 batches | ms/batch 32.08 | loss 2374.82\n",
      "| epoch  11 |  300/ 427 batches | ms/batch 31.84 | loss 2374.69\n",
      "| epoch  11 |  400/ 427 batches | ms/batch 32.01 | loss 2423.07\n",
      "Training took 13.67 seconds.\n",
      "Training phase...\n",
      "| epoch  12 |  100/ 427 batches | ms/batch 32.34 | loss 2414.08\n",
      "| epoch  12 |  200/ 427 batches | ms/batch 32.23 | loss 2348.11\n",
      "| epoch  12 |  300/ 427 batches | ms/batch 31.92 | loss 2423.77\n",
      "| epoch  12 |  400/ 427 batches | ms/batch 31.83 | loss 2417.38\n",
      "Training took 13.66 seconds.\n",
      "Training phase...\n",
      "| epoch  13 |  100/ 427 batches | ms/batch 32.39 | loss 2416.60\n",
      "| epoch  13 |  200/ 427 batches | ms/batch 32.01 | loss 2382.86\n",
      "| epoch  13 |  300/ 427 batches | ms/batch 32.06 | loss 2385.52\n",
      "| epoch  13 |  400/ 427 batches | ms/batch 32.05 | loss 2400.68\n",
      "Training took 13.66 seconds.\n",
      "Training phase...\n",
      "| epoch  14 |  100/ 427 batches | ms/batch 32.40 | loss 2404.27\n",
      "| epoch  14 |  200/ 427 batches | ms/batch 32.23 | loss 2371.67\n",
      "| epoch  14 |  300/ 427 batches | ms/batch 32.22 | loss 2419.27\n",
      "| epoch  14 |  400/ 427 batches | ms/batch 32.13 | loss 2384.40\n",
      "Training took 13.72 seconds.\n",
      "Training phase...\n",
      "| epoch  15 |  100/ 427 batches | ms/batch 32.76 | loss 2401.17\n",
      "| epoch  15 |  200/ 427 batches | ms/batch 32.19 | loss 2386.45\n",
      "| epoch  15 |  300/ 427 batches | ms/batch 32.10 | loss 2382.43\n",
      "| epoch  15 |  400/ 427 batches | ms/batch 32.22 | loss 2389.22\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  16 |  100/ 427 batches | ms/batch 32.58 | loss 2396.86\n",
      "| epoch  16 |  200/ 427 batches | ms/batch 32.08 | loss 2412.62\n",
      "| epoch  16 |  300/ 427 batches | ms/batch 32.03 | loss 2365.57\n",
      "| epoch  16 |  400/ 427 batches | ms/batch 31.69 | loss 2386.61\n",
      "Training took 13.66 seconds.\n",
      "Training phase...\n",
      "| epoch  17 |  100/ 427 batches | ms/batch 32.49 | loss 2400.88\n",
      "| epoch  17 |  200/ 427 batches | ms/batch 32.36 | loss 2386.36\n",
      "| epoch  17 |  300/ 427 batches | ms/batch 31.94 | loss 2389.31\n",
      "| epoch  17 |  400/ 427 batches | ms/batch 31.89 | loss 2389.47\n",
      "Training took 13.68 seconds.\n",
      "Training phase...\n",
      "| epoch  18 |  100/ 427 batches | ms/batch 32.30 | loss 2394.76\n",
      "| epoch  18 |  200/ 427 batches | ms/batch 32.03 | loss 2378.72\n",
      "| epoch  18 |  300/ 427 batches | ms/batch 31.99 | loss 2374.81\n",
      "| epoch  18 |  400/ 427 batches | ms/batch 32.04 | loss 2390.45\n",
      "Training took 13.67 seconds.\n",
      "Training phase...\n",
      "| epoch  19 |  100/ 427 batches | ms/batch 32.75 | loss 2389.46\n",
      "| epoch  19 |  200/ 427 batches | ms/batch 32.31 | loss 2377.08\n",
      "| epoch  19 |  300/ 427 batches | ms/batch 32.00 | loss 2361.16\n",
      "| epoch  19 |  400/ 427 batches | ms/batch 32.05 | loss 2414.34\n",
      "Training took 13.73 seconds.\n",
      "Training phase...\n",
      "| epoch  20 |  100/ 427 batches | ms/batch 32.29 | loss 2380.75\n",
      "| epoch  20 |  200/ 427 batches | ms/batch 32.12 | loss 2385.62\n",
      "| epoch  20 |  300/ 427 batches | ms/batch 32.09 | loss 2363.34\n",
      "| epoch  20 |  400/ 427 batches | ms/batch 32.17 | loss 2400.97\n",
      "Training took 13.68 seconds.\n",
      "Training phase...\n",
      "| epoch  21 |  100/ 427 batches | ms/batch 32.53 | loss 2427.02\n",
      "| epoch  21 |  200/ 427 batches | ms/batch 32.40 | loss 2373.28\n",
      "| epoch  21 |  300/ 427 batches | ms/batch 31.65 | loss 2367.63\n",
      "| epoch  21 |  400/ 427 batches | ms/batch 31.95 | loss 2352.20\n",
      "Training took 13.67 seconds.\n",
      "Training phase...\n",
      "| epoch  22 |  100/ 427 batches | ms/batch 32.36 | loss 2366.13\n",
      "| epoch  22 |  200/ 427 batches | ms/batch 31.98 | loss 2388.31\n",
      "| epoch  22 |  300/ 427 batches | ms/batch 32.10 | loss 2378.09\n",
      "| epoch  22 |  400/ 427 batches | ms/batch 32.10 | loss 2382.58\n",
      "Training took 13.67 seconds.\n",
      "Training phase...\n",
      "| epoch  23 |  100/ 427 batches | ms/batch 32.45 | loss 2407.37\n",
      "| epoch  23 |  200/ 427 batches | ms/batch 32.46 | loss 2368.61\n",
      "| epoch  23 |  300/ 427 batches | ms/batch 32.22 | loss 2361.12\n",
      "| epoch  23 |  400/ 427 batches | ms/batch 31.98 | loss 2379.39\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  24 |  100/ 427 batches | ms/batch 32.65 | loss 2381.02\n",
      "| epoch  24 |  200/ 427 batches | ms/batch 31.99 | loss 2357.98\n",
      "| epoch  24 |  300/ 427 batches | ms/batch 32.39 | loss 2382.95\n",
      "| epoch  24 |  400/ 427 batches | ms/batch 32.67 | loss 2382.91\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  25 |  100/ 427 batches | ms/batch 32.44 | loss 2401.72\n",
      "| epoch  25 |  200/ 427 batches | ms/batch 32.09 | loss 2372.17\n",
      "| epoch  25 |  300/ 427 batches | ms/batch 32.18 | loss 2349.48\n",
      "| epoch  25 |  400/ 427 batches | ms/batch 32.29 | loss 2391.82\n",
      "Training took 13.72 seconds.\n",
      "Training phase...\n",
      "| epoch  26 |  100/ 427 batches | ms/batch 32.46 | loss 2374.26\n",
      "| epoch  26 |  200/ 427 batches | ms/batch 31.73 | loss 2393.51\n",
      "| epoch  26 |  300/ 427 batches | ms/batch 32.11 | loss 2377.14\n",
      "| epoch  26 |  400/ 427 batches | ms/batch 32.26 | loss 2363.58\n",
      "Training took 13.68 seconds.\n",
      "Training phase...\n",
      "| epoch  27 |  100/ 427 batches | ms/batch 32.35 | loss 2438.19\n",
      "| epoch  27 |  200/ 427 batches | ms/batch 32.17 | loss 2352.02\n",
      "| epoch  27 |  300/ 427 batches | ms/batch 32.31 | loss 2368.35\n",
      "| epoch  27 |  400/ 427 batches | ms/batch 32.28 | loss 2344.70\n",
      "Training took 13.73 seconds.\n",
      "Training phase...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  28 |  100/ 427 batches | ms/batch 32.72 | loss 2365.72\n",
      "| epoch  28 |  200/ 427 batches | ms/batch 32.18 | loss 2369.63\n",
      "| epoch  28 |  300/ 427 batches | ms/batch 32.27 | loss 2369.14\n",
      "| epoch  28 |  400/ 427 batches | ms/batch 32.02 | loss 2387.41\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  29 |  100/ 427 batches | ms/batch 32.59 | loss 2385.68\n",
      "| epoch  29 |  200/ 427 batches | ms/batch 32.62 | loss 2393.44\n",
      "| epoch  29 |  300/ 427 batches | ms/batch 32.19 | loss 2340.95\n",
      "| epoch  29 |  400/ 427 batches | ms/batch 32.23 | loss 2381.32\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  30 |  100/ 427 batches | ms/batch 32.60 | loss 2386.78\n",
      "| epoch  30 |  200/ 427 batches | ms/batch 32.21 | loss 2329.53\n",
      "| epoch  30 |  300/ 427 batches | ms/batch 32.21 | loss 2389.92\n",
      "| epoch  30 |  400/ 427 batches | ms/batch 32.32 | loss 2363.82\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch  31 |  100/ 427 batches | ms/batch 32.16 | loss 2408.11\n",
      "| epoch  31 |  200/ 427 batches | ms/batch 32.05 | loss 2357.34\n",
      "| epoch  31 |  300/ 427 batches | ms/batch 32.34 | loss 2340.47\n",
      "| epoch  31 |  400/ 427 batches | ms/batch 32.35 | loss 2367.96\n",
      "Training took 13.72 seconds.\n",
      "Training phase...\n",
      "| epoch  32 |  100/ 427 batches | ms/batch 32.45 | loss 2386.36\n",
      "| epoch  32 |  200/ 427 batches | ms/batch 32.27 | loss 2348.36\n",
      "| epoch  32 |  300/ 427 batches | ms/batch 32.09 | loss 2364.45\n",
      "| epoch  32 |  400/ 427 batches | ms/batch 32.33 | loss 2377.44\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  33 |  100/ 427 batches | ms/batch 32.39 | loss 2366.90\n",
      "| epoch  33 |  200/ 427 batches | ms/batch 32.28 | loss 2374.96\n",
      "| epoch  33 |  300/ 427 batches | ms/batch 32.30 | loss 2357.88\n",
      "| epoch  33 |  400/ 427 batches | ms/batch 32.17 | loss 2364.29\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  34 |  100/ 427 batches | ms/batch 32.53 | loss 2405.19\n",
      "| epoch  34 |  200/ 427 batches | ms/batch 32.28 | loss 2331.51\n",
      "| epoch  34 |  300/ 427 batches | ms/batch 32.44 | loss 2349.74\n",
      "| epoch  34 |  400/ 427 batches | ms/batch 32.45 | loss 2373.84\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  35 |  100/ 427 batches | ms/batch 32.65 | loss 2381.34\n",
      "| epoch  35 |  200/ 427 batches | ms/batch 32.41 | loss 2358.50\n",
      "| epoch  35 |  300/ 427 batches | ms/batch 32.44 | loss 2398.21\n",
      "| epoch  35 |  400/ 427 batches | ms/batch 32.37 | loss 2320.33\n",
      "Training took 13.81 seconds.\n",
      "Training phase...\n",
      "| epoch  36 |  100/ 427 batches | ms/batch 32.54 | loss 2379.00\n",
      "| epoch  36 |  200/ 427 batches | ms/batch 32.01 | loss 2384.72\n",
      "| epoch  36 |  300/ 427 batches | ms/batch 32.22 | loss 2355.68\n",
      "| epoch  36 |  400/ 427 batches | ms/batch 32.14 | loss 2355.48\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  37 |  100/ 427 batches | ms/batch 32.51 | loss 2372.89\n",
      "| epoch  37 |  200/ 427 batches | ms/batch 32.16 | loss 2368.47\n",
      "| epoch  37 |  300/ 427 batches | ms/batch 32.14 | loss 2359.65\n",
      "| epoch  37 |  400/ 427 batches | ms/batch 32.05 | loss 2348.81\n",
      "Training took 13.73 seconds.\n",
      "Training phase...\n",
      "| epoch  38 |  100/ 427 batches | ms/batch 32.80 | loss 2395.83\n",
      "| epoch  38 |  200/ 427 batches | ms/batch 32.27 | loss 2352.21\n",
      "| epoch  38 |  300/ 427 batches | ms/batch 32.50 | loss 2358.54\n",
      "| epoch  38 |  400/ 427 batches | ms/batch 32.24 | loss 2351.23\n",
      "Training took 13.81 seconds.\n",
      "Training phase...\n",
      "| epoch  39 |  100/ 427 batches | ms/batch 32.75 | loss 2351.53\n",
      "| epoch  39 |  200/ 427 batches | ms/batch 32.20 | loss 2396.90\n",
      "| epoch  39 |  300/ 427 batches | ms/batch 32.09 | loss 2363.03\n",
      "| epoch  39 |  400/ 427 batches | ms/batch 32.17 | loss 2356.07\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  40 |  100/ 427 batches | ms/batch 32.61 | loss 2374.53\n",
      "| epoch  40 |  200/ 427 batches | ms/batch 32.22 | loss 2351.42\n",
      "| epoch  40 |  300/ 427 batches | ms/batch 32.28 | loss 2357.85\n",
      "| epoch  40 |  400/ 427 batches | ms/batch 32.51 | loss 2355.90\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  41 |  100/ 427 batches | ms/batch 32.45 | loss 2353.82\n",
      "| epoch  41 |  200/ 427 batches | ms/batch 32.41 | loss 2365.97\n",
      "| epoch  41 |  300/ 427 batches | ms/batch 32.23 | loss 2370.95\n",
      "| epoch  41 |  400/ 427 batches | ms/batch 32.13 | loss 2358.39\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  42 |  100/ 427 batches | ms/batch 32.55 | loss 2321.68\n",
      "| epoch  42 |  200/ 427 batches | ms/batch 32.43 | loss 2350.48\n",
      "| epoch  42 |  300/ 427 batches | ms/batch 32.58 | loss 2350.61\n",
      "| epoch  42 |  400/ 427 batches | ms/batch 32.42 | loss 2404.04\n",
      "Training took 13.83 seconds.\n",
      "Training phase...\n",
      "| epoch  43 |  100/ 427 batches | ms/batch 32.66 | loss 2374.49\n",
      "| epoch  43 |  200/ 427 batches | ms/batch 32.28 | loss 2352.18\n",
      "| epoch  43 |  300/ 427 batches | ms/batch 32.27 | loss 2348.51\n",
      "| epoch  43 |  400/ 427 batches | ms/batch 32.17 | loss 2364.50\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  44 |  100/ 427 batches | ms/batch 32.50 | loss 2365.85\n",
      "| epoch  44 |  200/ 427 batches | ms/batch 32.08 | loss 2346.17\n",
      "| epoch  44 |  300/ 427 batches | ms/batch 31.90 | loss 2375.43\n",
      "| epoch  44 |  400/ 427 batches | ms/batch 32.31 | loss 2367.08\n",
      "Training took 13.7 seconds.\n",
      "Training phase...\n",
      "| epoch  45 |  100/ 427 batches | ms/batch 32.80 | loss 2387.00\n",
      "| epoch  45 |  200/ 427 batches | ms/batch 32.38 | loss 2350.14\n",
      "| epoch  45 |  300/ 427 batches | ms/batch 32.35 | loss 2356.30\n",
      "| epoch  45 |  400/ 427 batches | ms/batch 32.16 | loss 2361.74\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  46 |  100/ 427 batches | ms/batch 32.47 | loss 2371.80\n",
      "| epoch  46 |  200/ 427 batches | ms/batch 32.18 | loss 2364.66\n",
      "| epoch  46 |  300/ 427 batches | ms/batch 32.47 | loss 2356.99\n",
      "| epoch  46 |  400/ 427 batches | ms/batch 32.15 | loss 2346.25\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  47 |  100/ 427 batches | ms/batch 32.65 | loss 2376.53\n",
      "| epoch  47 |  200/ 427 batches | ms/batch 32.29 | loss 2341.41\n",
      "| epoch  47 |  300/ 427 batches | ms/batch 32.36 | loss 2358.28\n",
      "| epoch  47 |  400/ 427 batches | ms/batch 32.19 | loss 2379.04\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  48 |  100/ 427 batches | ms/batch 32.47 | loss 2374.36\n",
      "| epoch  48 |  200/ 427 batches | ms/batch 32.01 | loss 2363.64\n",
      "| epoch  48 |  300/ 427 batches | ms/batch 32.24 | loss 2354.79\n",
      "| epoch  48 |  400/ 427 batches | ms/batch 32.33 | loss 2346.50\n",
      "Training took 13.72 seconds.\n",
      "Training phase...\n",
      "| epoch  49 |  100/ 427 batches | ms/batch 32.66 | loss 2395.50\n",
      "| epoch  49 |  200/ 427 batches | ms/batch 32.22 | loss 2332.19\n",
      "| epoch  49 |  300/ 427 batches | ms/batch 32.34 | loss 2344.14\n",
      "| epoch  49 |  400/ 427 batches | ms/batch 32.24 | loss 2360.61\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch  50 |  100/ 427 batches | ms/batch 32.50 | loss 2390.27\n",
      "| epoch  50 |  200/ 427 batches | ms/batch 32.29 | loss 2338.78\n",
      "| epoch  50 |  300/ 427 batches | ms/batch 32.29 | loss 2330.68\n",
      "| epoch  50 |  400/ 427 batches | ms/batch 31.79 | loss 2386.61\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  51 |  100/ 427 batches | ms/batch 32.97 | loss 2379.02\n",
      "| epoch  51 |  200/ 427 batches | ms/batch 32.36 | loss 2356.73\n",
      "| epoch  51 |  300/ 427 batches | ms/batch 32.17 | loss 2358.72\n",
      "| epoch  51 |  400/ 427 batches | ms/batch 32.18 | loss 2333.17\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  52 |  100/ 427 batches | ms/batch 32.66 | loss 2408.15\n",
      "| epoch  52 |  200/ 427 batches | ms/batch 32.10 | loss 2352.77\n",
      "| epoch  52 |  300/ 427 batches | ms/batch 32.22 | loss 2338.51\n",
      "| epoch  52 |  400/ 427 batches | ms/batch 32.16 | loss 2339.17\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  53 |  100/ 427 batches | ms/batch 32.62 | loss 2357.98\n",
      "| epoch  53 |  200/ 427 batches | ms/batch 32.52 | loss 2368.37\n",
      "| epoch  53 |  300/ 427 batches | ms/batch 32.24 | loss 2340.95\n",
      "| epoch  53 |  400/ 427 batches | ms/batch 32.62 | loss 2359.46\n",
      "Training took 13.83 seconds.\n",
      "Training phase...\n",
      "| epoch  54 |  100/ 427 batches | ms/batch 32.88 | loss 2383.80\n",
      "| epoch  54 |  200/ 427 batches | ms/batch 32.48 | loss 2350.99\n",
      "| epoch  54 |  300/ 427 batches | ms/batch 32.47 | loss 2342.45\n",
      "| epoch  54 |  400/ 427 batches | ms/batch 32.47 | loss 2356.05\n",
      "Training took 13.85 seconds.\n",
      "Training phase...\n",
      "| epoch  55 |  100/ 427 batches | ms/batch 32.73 | loss 2363.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  55 |  200/ 427 batches | ms/batch 32.65 | loss 2374.60\n",
      "| epoch  55 |  300/ 427 batches | ms/batch 32.26 | loss 2317.88\n",
      "| epoch  55 |  400/ 427 batches | ms/batch 32.03 | loss 2367.90\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  56 |  100/ 427 batches | ms/batch 32.57 | loss 2356.31\n",
      "| epoch  56 |  200/ 427 batches | ms/batch 32.07 | loss 2338.63\n",
      "| epoch  56 |  300/ 427 batches | ms/batch 32.55 | loss 2401.13\n",
      "| epoch  56 |  400/ 427 batches | ms/batch 32.31 | loss 2330.42\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  57 |  100/ 427 batches | ms/batch 32.51 | loss 2384.65\n",
      "| epoch  57 |  200/ 427 batches | ms/batch 32.16 | loss 2380.41\n",
      "| epoch  57 |  300/ 427 batches | ms/batch 32.48 | loss 2337.67\n",
      "| epoch  57 |  400/ 427 batches | ms/batch 32.10 | loss 2350.59\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  58 |  100/ 427 batches | ms/batch 32.59 | loss 2364.71\n",
      "| epoch  58 |  200/ 427 batches | ms/batch 32.45 | loss 2367.11\n",
      "| epoch  58 |  300/ 427 batches | ms/batch 32.40 | loss 2348.24\n",
      "| epoch  58 |  400/ 427 batches | ms/batch 32.45 | loss 2359.39\n",
      "Training took 13.82 seconds.\n",
      "Training phase...\n",
      "| epoch  59 |  100/ 427 batches | ms/batch 32.87 | loss 2384.34\n",
      "| epoch  59 |  200/ 427 batches | ms/batch 32.04 | loss 2363.82\n",
      "| epoch  59 |  300/ 427 batches | ms/batch 32.23 | loss 2332.91\n",
      "| epoch  59 |  400/ 427 batches | ms/batch 32.29 | loss 2346.86\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  60 |  100/ 427 batches | ms/batch 32.79 | loss 2349.87\n",
      "| epoch  60 |  200/ 427 batches | ms/batch 32.27 | loss 2349.42\n",
      "| epoch  60 |  300/ 427 batches | ms/batch 32.37 | loss 2353.74\n",
      "| epoch  60 |  400/ 427 batches | ms/batch 32.28 | loss 2370.77\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  61 |  100/ 427 batches | ms/batch 32.32 | loss 2361.47\n",
      "| epoch  61 |  200/ 427 batches | ms/batch 32.52 | loss 2358.79\n",
      "| epoch  61 |  300/ 427 batches | ms/batch 32.43 | loss 2363.45\n",
      "| epoch  61 |  400/ 427 batches | ms/batch 32.35 | loss 2344.14\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  62 |  100/ 427 batches | ms/batch 32.66 | loss 2381.39\n",
      "| epoch  62 |  200/ 427 batches | ms/batch 32.27 | loss 2356.38\n",
      "| epoch  62 |  300/ 427 batches | ms/batch 32.62 | loss 2342.72\n",
      "| epoch  62 |  400/ 427 batches | ms/batch 32.20 | loss 2360.01\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  63 |  100/ 427 batches | ms/batch 32.80 | loss 2381.47\n",
      "| epoch  63 |  200/ 427 batches | ms/batch 32.57 | loss 2356.79\n",
      "| epoch  63 |  300/ 427 batches | ms/batch 32.14 | loss 2344.09\n",
      "| epoch  63 |  400/ 427 batches | ms/batch 32.59 | loss 2348.20\n",
      "Training took 13.84 seconds.\n",
      "Training phase...\n",
      "| epoch  64 |  100/ 427 batches | ms/batch 33.16 | loss 2354.54\n",
      "| epoch  64 |  200/ 427 batches | ms/batch 32.50 | loss 2341.55\n",
      "| epoch  64 |  300/ 427 batches | ms/batch 32.23 | loss 2350.51\n",
      "| epoch  64 |  400/ 427 batches | ms/batch 32.58 | loss 2373.56\n",
      "Training took 13.87 seconds.\n",
      "Training phase...\n",
      "| epoch  65 |  100/ 427 batches | ms/batch 32.79 | loss 2384.42\n",
      "| epoch  65 |  200/ 427 batches | ms/batch 32.20 | loss 2361.23\n",
      "| epoch  65 |  300/ 427 batches | ms/batch 31.99 | loss 2334.21\n",
      "| epoch  65 |  400/ 427 batches | ms/batch 32.34 | loss 2335.75\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  66 |  100/ 427 batches | ms/batch 32.42 | loss 2363.39\n",
      "| epoch  66 |  200/ 427 batches | ms/batch 32.19 | loss 2362.61\n",
      "| epoch  66 |  300/ 427 batches | ms/batch 32.19 | loss 2363.09\n",
      "| epoch  66 |  400/ 427 batches | ms/batch 32.13 | loss 2329.86\n",
      "Training took 13.73 seconds.\n",
      "Training phase...\n",
      "| epoch  67 |  100/ 427 batches | ms/batch 32.81 | loss 2354.93\n",
      "| epoch  67 |  200/ 427 batches | ms/batch 32.38 | loss 2411.49\n",
      "| epoch  67 |  300/ 427 batches | ms/batch 32.32 | loss 2320.31\n",
      "| epoch  67 |  400/ 427 batches | ms/batch 32.20 | loss 2344.09\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  68 |  100/ 427 batches | ms/batch 32.74 | loss 2370.73\n",
      "| epoch  68 |  200/ 427 batches | ms/batch 32.33 | loss 2323.54\n",
      "| epoch  68 |  300/ 427 batches | ms/batch 32.22 | loss 2358.73\n",
      "| epoch  68 |  400/ 427 batches | ms/batch 32.45 | loss 2369.94\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  69 |  100/ 427 batches | ms/batch 32.67 | loss 2349.92\n",
      "| epoch  69 |  200/ 427 batches | ms/batch 32.29 | loss 2345.24\n",
      "| epoch  69 |  300/ 427 batches | ms/batch 32.19 | loss 2379.37\n",
      "| epoch  69 |  400/ 427 batches | ms/batch 32.29 | loss 2353.84\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  70 |  100/ 427 batches | ms/batch 32.90 | loss 2364.91\n",
      "| epoch  70 |  200/ 427 batches | ms/batch 32.02 | loss 2348.54\n",
      "| epoch  70 |  300/ 427 batches | ms/batch 32.10 | loss 2357.14\n",
      "| epoch  70 |  400/ 427 batches | ms/batch 32.25 | loss 2360.01\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  71 |  100/ 427 batches | ms/batch 32.68 | loss 2349.67\n",
      "| epoch  71 |  200/ 427 batches | ms/batch 32.20 | loss 2345.59\n",
      "| epoch  71 |  300/ 427 batches | ms/batch 32.14 | loss 2365.82\n",
      "| epoch  71 |  400/ 427 batches | ms/batch 32.27 | loss 2352.45\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  72 |  100/ 427 batches | ms/batch 32.74 | loss 2374.78\n",
      "| epoch  72 |  200/ 427 batches | ms/batch 32.06 | loss 2352.67\n",
      "| epoch  72 |  300/ 427 batches | ms/batch 32.12 | loss 2349.79\n",
      "| epoch  72 |  400/ 427 batches | ms/batch 32.62 | loss 2344.95\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  73 |  100/ 427 batches | ms/batch 32.68 | loss 2362.17\n",
      "| epoch  73 |  200/ 427 batches | ms/batch 32.44 | loss 2351.14\n",
      "| epoch  73 |  300/ 427 batches | ms/batch 32.41 | loss 2365.44\n",
      "| epoch  73 |  400/ 427 batches | ms/batch 32.25 | loss 2328.33\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  74 |  100/ 427 batches | ms/batch 32.78 | loss 2360.93\n",
      "| epoch  74 |  200/ 427 batches | ms/batch 32.49 | loss 2357.90\n",
      "| epoch  74 |  300/ 427 batches | ms/batch 32.61 | loss 2358.45\n",
      "| epoch  74 |  400/ 427 batches | ms/batch 32.31 | loss 2342.48\n",
      "Training took 13.85 seconds.\n",
      "Training phase...\n",
      "| epoch  75 |  100/ 427 batches | ms/batch 32.51 | loss 2377.31\n",
      "| epoch  75 |  200/ 427 batches | ms/batch 32.38 | loss 2325.69\n",
      "| epoch  75 |  300/ 427 batches | ms/batch 32.11 | loss 2332.26\n",
      "| epoch  75 |  400/ 427 batches | ms/batch 32.27 | loss 2379.89\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  76 |  100/ 427 batches | ms/batch 32.77 | loss 2394.00\n",
      "| epoch  76 |  200/ 427 batches | ms/batch 32.29 | loss 2313.63\n",
      "| epoch  76 |  300/ 427 batches | ms/batch 32.38 | loss 2344.21\n",
      "| epoch  76 |  400/ 427 batches | ms/batch 32.19 | loss 2347.43\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  77 |  100/ 427 batches | ms/batch 32.81 | loss 2363.96\n",
      "| epoch  77 |  200/ 427 batches | ms/batch 32.45 | loss 2343.31\n",
      "| epoch  77 |  300/ 427 batches | ms/batch 32.30 | loss 2335.10\n",
      "| epoch  77 |  400/ 427 batches | ms/batch 32.43 | loss 2364.13\n",
      "Training took 13.83 seconds.\n",
      "Training phase...\n",
      "| epoch  78 |  100/ 427 batches | ms/batch 32.48 | loss 2343.47\n",
      "| epoch  78 |  200/ 427 batches | ms/batch 32.36 | loss 2335.10\n",
      "| epoch  78 |  300/ 427 batches | ms/batch 32.32 | loss 2361.22\n",
      "| epoch  78 |  400/ 427 batches | ms/batch 32.23 | loss 2382.35\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch  79 |  100/ 427 batches | ms/batch 32.68 | loss 2344.72\n",
      "| epoch  79 |  200/ 427 batches | ms/batch 32.29 | loss 2332.93\n",
      "| epoch  79 |  300/ 427 batches | ms/batch 32.19 | loss 2354.65\n",
      "| epoch  79 |  400/ 427 batches | ms/batch 32.00 | loss 2366.89\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  80 |  100/ 427 batches | ms/batch 32.59 | loss 2381.04\n",
      "| epoch  80 |  200/ 427 batches | ms/batch 32.01 | loss 2329.99\n",
      "| epoch  80 |  300/ 427 batches | ms/batch 32.20 | loss 2368.80\n",
      "| epoch  80 |  400/ 427 batches | ms/batch 32.27 | loss 2331.88\n",
      "Training took 13.73 seconds.\n",
      "Training phase...\n",
      "| epoch  81 |  100/ 427 batches | ms/batch 32.52 | loss 2363.06\n",
      "| epoch  81 |  200/ 427 batches | ms/batch 32.27 | loss 2327.94\n",
      "| epoch  81 |  300/ 427 batches | ms/batch 32.48 | loss 2335.06\n",
      "| epoch  81 |  400/ 427 batches | ms/batch 32.06 | loss 2380.25\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch  82 |  100/ 427 batches | ms/batch 32.50 | loss 2372.93\n",
      "| epoch  82 |  200/ 427 batches | ms/batch 32.05 | loss 2337.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  82 |  300/ 427 batches | ms/batch 32.12 | loss 2349.16\n",
      "| epoch  82 |  400/ 427 batches | ms/batch 32.22 | loss 2343.39\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  83 |  100/ 427 batches | ms/batch 32.59 | loss 2370.23\n",
      "| epoch  83 |  200/ 427 batches | ms/batch 32.47 | loss 2341.35\n",
      "| epoch  83 |  300/ 427 batches | ms/batch 32.33 | loss 2358.50\n",
      "| epoch  83 |  400/ 427 batches | ms/batch 32.38 | loss 2344.86\n",
      "Training took 13.81 seconds.\n",
      "Training phase...\n",
      "| epoch  84 |  100/ 427 batches | ms/batch 32.52 | loss 2370.24\n",
      "| epoch  84 |  200/ 427 batches | ms/batch 32.41 | loss 2329.12\n",
      "| epoch  84 |  300/ 427 batches | ms/batch 32.14 | loss 2370.52\n",
      "| epoch  84 |  400/ 427 batches | ms/batch 32.23 | loss 2338.68\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  85 |  100/ 427 batches | ms/batch 32.87 | loss 2371.15\n",
      "| epoch  85 |  200/ 427 batches | ms/batch 32.12 | loss 2326.60\n",
      "| epoch  85 |  300/ 427 batches | ms/batch 31.97 | loss 2364.56\n",
      "| epoch  85 |  400/ 427 batches | ms/batch 32.32 | loss 2346.50\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  86 |  100/ 427 batches | ms/batch 32.73 | loss 2332.41\n",
      "| epoch  86 |  200/ 427 batches | ms/batch 32.35 | loss 2373.15\n",
      "| epoch  86 |  300/ 427 batches | ms/batch 32.18 | loss 2348.28\n",
      "| epoch  86 |  400/ 427 batches | ms/batch 32.15 | loss 2341.62\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  87 |  100/ 427 batches | ms/batch 32.76 | loss 2358.61\n",
      "| epoch  87 |  200/ 427 batches | ms/batch 32.18 | loss 2365.46\n",
      "| epoch  87 |  300/ 427 batches | ms/batch 32.25 | loss 2326.08\n",
      "| epoch  87 |  400/ 427 batches | ms/batch 32.39 | loss 2358.09\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  88 |  100/ 427 batches | ms/batch 32.87 | loss 2389.29\n",
      "| epoch  88 |  200/ 427 batches | ms/batch 32.41 | loss 2326.37\n",
      "| epoch  88 |  300/ 427 batches | ms/batch 32.40 | loss 2355.21\n",
      "| epoch  88 |  400/ 427 batches | ms/batch 32.51 | loss 2341.00\n",
      "Training took 13.85 seconds.\n",
      "Training phase...\n",
      "| epoch  89 |  100/ 427 batches | ms/batch 32.65 | loss 2341.76\n",
      "| epoch  89 |  200/ 427 batches | ms/batch 32.26 | loss 2344.81\n",
      "| epoch  89 |  300/ 427 batches | ms/batch 32.16 | loss 2363.48\n",
      "| epoch  89 |  400/ 427 batches | ms/batch 32.47 | loss 2347.37\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  90 |  100/ 427 batches | ms/batch 32.87 | loss 2360.88\n",
      "| epoch  90 |  200/ 427 batches | ms/batch 32.11 | loss 2336.18\n",
      "| epoch  90 |  300/ 427 batches | ms/batch 32.15 | loss 2353.43\n",
      "| epoch  90 |  400/ 427 batches | ms/batch 32.10 | loss 2350.55\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  91 |  100/ 427 batches | ms/batch 32.66 | loss 2373.87\n",
      "| epoch  91 |  200/ 427 batches | ms/batch 32.25 | loss 2342.12\n",
      "| epoch  91 |  300/ 427 batches | ms/batch 32.57 | loss 2337.61\n",
      "| epoch  91 |  400/ 427 batches | ms/batch 32.20 | loss 2349.64\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  92 |  100/ 427 batches | ms/batch 32.71 | loss 2376.73\n",
      "| epoch  92 |  200/ 427 batches | ms/batch 32.29 | loss 2314.17\n",
      "| epoch  92 |  300/ 427 batches | ms/batch 32.25 | loss 2377.74\n",
      "| epoch  92 |  400/ 427 batches | ms/batch 32.08 | loss 2351.93\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  93 |  100/ 427 batches | ms/batch 32.86 | loss 2389.83\n",
      "| epoch  93 |  200/ 427 batches | ms/batch 32.17 | loss 2326.54\n",
      "| epoch  93 |  300/ 427 batches | ms/batch 32.34 | loss 2331.60\n",
      "| epoch  93 |  400/ 427 batches | ms/batch 32.17 | loss 2342.13\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  94 |  100/ 427 batches | ms/batch 32.95 | loss 2385.91\n",
      "| epoch  94 |  200/ 427 batches | ms/batch 32.47 | loss 2331.83\n",
      "| epoch  94 |  300/ 427 batches | ms/batch 32.70 | loss 2319.65\n",
      "| epoch  94 |  400/ 427 batches | ms/batch 32.79 | loss 2365.56\n",
      "Training took 13.91 seconds.\n",
      "Training phase...\n",
      "| epoch  95 |  100/ 427 batches | ms/batch 32.50 | loss 2345.92\n",
      "| epoch  95 |  200/ 427 batches | ms/batch 31.82 | loss 2337.05\n",
      "| epoch  95 |  300/ 427 batches | ms/batch 32.26 | loss 2377.78\n",
      "| epoch  95 |  400/ 427 batches | ms/batch 32.18 | loss 2349.53\n",
      "Training took 13.7 seconds.\n",
      "Training phase...\n",
      "| epoch  96 |  100/ 427 batches | ms/batch 32.43 | loss 2383.88\n",
      "| epoch  96 |  200/ 427 batches | ms/batch 32.35 | loss 2331.46\n",
      "| epoch  96 |  300/ 427 batches | ms/batch 32.35 | loss 2337.63\n",
      "| epoch  96 |  400/ 427 batches | ms/batch 32.28 | loss 2350.47\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  97 |  100/ 427 batches | ms/batch 32.80 | loss 2351.66\n",
      "| epoch  97 |  200/ 427 batches | ms/batch 32.44 | loss 2341.93\n",
      "| epoch  97 |  300/ 427 batches | ms/batch 32.12 | loss 2356.19\n",
      "| epoch  97 |  400/ 427 batches | ms/batch 32.38 | loss 2367.08\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  98 |  100/ 427 batches | ms/batch 32.77 | loss 2370.16\n",
      "| epoch  98 |  200/ 427 batches | ms/batch 32.46 | loss 2339.53\n",
      "| epoch  98 |  300/ 427 batches | ms/batch 32.42 | loss 2340.75\n",
      "| epoch  98 |  400/ 427 batches | ms/batch 32.44 | loss 2334.40\n",
      "Training took 13.85 seconds.\n",
      "Training phase...\n",
      "| epoch  99 |  100/ 427 batches | ms/batch 32.49 | loss 2365.28\n",
      "| epoch  99 |  200/ 427 batches | ms/batch 32.62 | loss 2334.31\n",
      "| epoch  99 |  300/ 427 batches | ms/batch 32.30 | loss 2352.38\n",
      "| epoch  99 |  400/ 427 batches | ms/batch 32.32 | loss 2360.35\n",
      "Training took 13.81 seconds.\n",
      "Training phase...\n",
      "| epoch 100 |  100/ 427 batches | ms/batch 32.59 | loss 2357.80\n",
      "| epoch 100 |  200/ 427 batches | ms/batch 32.29 | loss 2355.82\n",
      "| epoch 100 |  300/ 427 batches | ms/batch 32.27 | loss 2343.90\n",
      "| epoch 100 |  400/ 427 batches | ms/batch 32.32 | loss 2366.20\n",
      "Training took 13.78 seconds.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1757/2616378239.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m           k=train_params[\"k\"], beta=train_params[\"beta\"])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mndcg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplicit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/utils_VAE.py\u001b[0m in \u001b[0;36mpredict_metrics\u001b[0;34m(self, train_data, test_data)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mndcg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mndcg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/utils_VAE.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, train_data, val_data)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mdata_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaive_sparse2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/utils_VAE.py\u001b[0m in \u001b[0;36mnaive_sparse2tensor\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnaive_sparse2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "model = TrainableMultVAE(base_params[\"encoder_dims\"], base_params[\"decoder_dims\"], base_params[\"dropout\"])\n",
    "optimizer = optim.Adam(model.parameters(), **train_params[\"optimizer_kwargs\"])\n",
    "\n",
    "model.fit(train_data, optimizer, criterion, val_data=None, n_epochs=train_params[\"n_epochs\"],\n",
    "          k=train_params[\"k\"], beta=train_params[\"beta\"])\n",
    "\n",
    "ndcg, err, hr = model.predict_metrics(train_data, test_data)\n",
    "\n",
    "res = {}\n",
    "res[\"NDCG10\"] = ndcg\n",
    "res[\"ERR10\"] = err\n",
    "res[\"HR10\"] = hr\n",
    "res[\"dataset\"] = \"explicit\"\n",
    "\n",
    "with open(tmp_savepath+\"vae_explicit\", \"wb\") as handle:\n",
    "    pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda534c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "debf686a",
   "metadata": {},
   "source": [
    "### SLIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "645a2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_slim = pd.read_pickle(\"/home/mmarzec12/models/slim/\"+\"slim_best_params\")\n",
    "tmp_savepath_slim = \"/home/mmarzec12/models/slim/dataset_impact\"\n",
    "\n",
    "l1_reg = best_params_slim[\"l1_reg\"]\n",
    "l2_reg = best_params_slim[\"l2_reg\"]\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00ec99ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning all 2265 vectors took 5.41 minutes.\n",
      "In W matrix we have 88378 nonzero elements (1.723%).\n",
      "Computing top-k list for each user...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8925c03ffe4c9bb34e082e69cda811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...evaluation...\n",
      "0.18944588067206375 0.15648312091511826 0.29664295405375674\n"
     ]
    }
   ],
   "source": [
    "# set the parameters\n",
    "slim = ParallSynSLIM(l1_reg, l2_reg)\n",
    "\n",
    "# train the model\n",
    "slim.fit(train_data)\n",
    "\n",
    "# how many nonzero entires in W matirx\n",
    "proc = 100*slim.W.nnz/slim.W.shape[0]**2\n",
    "\n",
    "print(\"Computing top-k list for each user...\")\n",
    "# produce top k list for all users\n",
    "start = time.time()\n",
    "top_k_list = slim.calculate_top_k(train_data, ids_to_gs, ids_to_us, k=k)\n",
    "pred_time = time.time() - start\n",
    "\n",
    "print(\"...evaluation...\")\n",
    "ev = Evaluator(k=k, true=test_list, predicted=top_k_list)\n",
    "ev.calculate_metrics()\n",
    "ngcg10, err10, hr10 = ev.ndcg, ev.err, ev.hr\n",
    "\n",
    "# save the obtained results\n",
    "\n",
    "res = {}\n",
    "res[\"ndcg10\"] = ngcg10\n",
    "res[\"err10\"] = err10\n",
    "res[\"hr10\"] = hr10\n",
    "res[\"W_zeros_percentage\"] = proc\n",
    "res[\"prediction_calc_time_seconds\"] = pred_time\n",
    "res[\"datset\"] = \"explicit\"\n",
    "\n",
    "\n",
    "print(ngcg10, err10, hr10)\n",
    "# save the obtained results\n",
    "with open(tmp_savepath_slim+\"SLIM_explicit\", \"wb\") as handle:\n",
    "    pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4fd56",
   "metadata": {},
   "source": [
    "## Binarized explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54c99e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit dataset\n",
    "explicit_binarized = explicit.copy()\n",
    "explicit_binarized.score = 1\n",
    "row = [us_to_ids[us] for us in explicit_binarized.user_name]\n",
    "col = [gs_to_ids[g] for g in explicit_binarized.game_id]\n",
    "data = explicit_binarized.score\n",
    "\n",
    "train_data = scipy.sparse.coo_matrix((data, (row, col)), shape=(len(unique_users), len(unique_games))).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef9fef",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e2f1df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase...\n",
      "| epoch   1 |  100/ 427 batches | ms/batch 32.98 | loss 397.87\n",
      "| epoch   1 |  200/ 427 batches | ms/batch 32.22 | loss 368.92\n",
      "| epoch   1 |  300/ 427 batches | ms/batch 32.28 | loss 371.12\n",
      "| epoch   1 |  400/ 427 batches | ms/batch 32.55 | loss 362.93\n",
      "Training took 13.82 seconds.\n",
      "Training phase...\n",
      "| epoch   2 |  100/ 427 batches | ms/batch 32.55 | loss 368.02\n",
      "| epoch   2 |  200/ 427 batches | ms/batch 32.13 | loss 362.16\n",
      "| epoch   2 |  300/ 427 batches | ms/batch 32.35 | loss 356.80\n",
      "| epoch   2 |  400/ 427 batches | ms/batch 32.30 | loss 359.20\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch   3 |  100/ 427 batches | ms/batch 32.28 | loss 365.68\n",
      "| epoch   3 |  200/ 427 batches | ms/batch 32.20 | loss 352.89\n",
      "| epoch   3 |  300/ 427 batches | ms/batch 32.22 | loss 357.77\n",
      "| epoch   3 |  400/ 427 batches | ms/batch 32.24 | loss 353.65\n",
      "Training took 13.72 seconds.\n",
      "Training phase...\n",
      "| epoch   4 |  100/ 427 batches | ms/batch 32.53 | loss 356.63\n",
      "| epoch   4 |  200/ 427 batches | ms/batch 32.14 | loss 352.22\n",
      "| epoch   4 |  300/ 427 batches | ms/batch 32.23 | loss 357.81\n",
      "| epoch   4 |  400/ 427 batches | ms/batch 32.35 | loss 356.46\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch   5 |  100/ 427 batches | ms/batch 32.41 | loss 357.07\n",
      "| epoch   5 |  200/ 427 batches | ms/batch 32.39 | loss 347.57\n",
      "| epoch   5 |  300/ 427 batches | ms/batch 32.38 | loss 355.77\n",
      "| epoch   5 |  400/ 427 batches | ms/batch 32.28 | loss 353.87\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch   6 |  100/ 427 batches | ms/batch 32.70 | loss 355.85\n",
      "| epoch   6 |  200/ 427 batches | ms/batch 32.23 | loss 351.96\n",
      "| epoch   6 |  300/ 427 batches | ms/batch 32.16 | loss 350.61\n",
      "| epoch   6 |  400/ 427 batches | ms/batch 32.29 | loss 349.24\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch   7 |  100/ 427 batches | ms/batch 32.50 | loss 358.94\n",
      "| epoch   7 |  200/ 427 batches | ms/batch 32.32 | loss 345.54\n",
      "| epoch   7 |  300/ 427 batches | ms/batch 32.45 | loss 348.61\n",
      "| epoch   7 |  400/ 427 batches | ms/batch 32.31 | loss 349.98\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch   8 |  100/ 427 batches | ms/batch 32.25 | loss 351.73\n",
      "| epoch   8 |  200/ 427 batches | ms/batch 32.13 | loss 347.08\n",
      "| epoch   8 |  300/ 427 batches | ms/batch 32.11 | loss 349.31\n",
      "| epoch   8 |  400/ 427 batches | ms/batch 32.17 | loss 349.17\n",
      "Training took 13.68 seconds.\n",
      "Training phase...\n",
      "| epoch   9 |  100/ 427 batches | ms/batch 32.49 | loss 349.73\n",
      "| epoch   9 |  200/ 427 batches | ms/batch 32.29 | loss 349.10\n",
      "| epoch   9 |  300/ 427 batches | ms/batch 32.18 | loss 349.05\n",
      "| epoch   9 |  400/ 427 batches | ms/batch 32.20 | loss 347.88\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  10 |  100/ 427 batches | ms/batch 32.59 | loss 353.35\n",
      "| epoch  10 |  200/ 427 batches | ms/batch 32.14 | loss 345.73\n",
      "| epoch  10 |  300/ 427 batches | ms/batch 32.21 | loss 347.28\n",
      "| epoch  10 |  400/ 427 batches | ms/batch 32.25 | loss 345.96\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  11 |  100/ 427 batches | ms/batch 32.50 | loss 351.39\n",
      "| epoch  11 |  200/ 427 batches | ms/batch 32.23 | loss 345.89\n",
      "| epoch  11 |  300/ 427 batches | ms/batch 32.37 | loss 347.18\n",
      "| epoch  11 |  400/ 427 batches | ms/batch 32.40 | loss 344.83\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  12 |  100/ 427 batches | ms/batch 33.78 | loss 350.19\n",
      "| epoch  12 |  200/ 427 batches | ms/batch 32.36 | loss 345.23\n",
      "| epoch  12 |  300/ 427 batches | ms/batch 32.33 | loss 347.04\n",
      "| epoch  12 |  400/ 427 batches | ms/batch 32.13 | loss 342.39\n",
      "Training took 13.87 seconds.\n",
      "Training phase...\n",
      "| epoch  13 |  100/ 427 batches | ms/batch 32.39 | loss 347.05\n",
      "| epoch  13 |  200/ 427 batches | ms/batch 32.09 | loss 346.46\n",
      "| epoch  13 |  300/ 427 batches | ms/batch 32.24 | loss 345.74\n",
      "| epoch  13 |  400/ 427 batches | ms/batch 32.14 | loss 346.31\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  14 |  100/ 427 batches | ms/batch 32.57 | loss 347.36\n",
      "| epoch  14 |  200/ 427 batches | ms/batch 32.37 | loss 349.09\n",
      "| epoch  14 |  300/ 427 batches | ms/batch 32.12 | loss 344.70\n",
      "| epoch  14 |  400/ 427 batches | ms/batch 32.15 | loss 344.07\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  15 |  100/ 427 batches | ms/batch 32.68 | loss 353.18\n",
      "| epoch  15 |  200/ 427 batches | ms/batch 32.33 | loss 344.19\n",
      "| epoch  15 |  300/ 427 batches | ms/batch 32.11 | loss 341.85\n",
      "| epoch  15 |  400/ 427 batches | ms/batch 32.22 | loss 342.84\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  16 |  100/ 427 batches | ms/batch 32.58 | loss 346.84\n",
      "| epoch  16 |  200/ 427 batches | ms/batch 32.70 | loss 345.33\n",
      "| epoch  16 |  300/ 427 batches | ms/batch 32.33 | loss 342.70\n",
      "| epoch  16 |  400/ 427 batches | ms/batch 32.39 | loss 346.55\n",
      "Training took 13.82 seconds.\n",
      "Training phase...\n",
      "| epoch  17 |  100/ 427 batches | ms/batch 32.64 | loss 342.25\n",
      "| epoch  17 |  200/ 427 batches | ms/batch 32.36 | loss 344.27\n",
      "| epoch  17 |  300/ 427 batches | ms/batch 32.30 | loss 344.14\n",
      "| epoch  17 |  400/ 427 batches | ms/batch 32.01 | loss 349.35\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  18 |  100/ 427 batches | ms/batch 32.42 | loss 347.23\n",
      "| epoch  18 |  200/ 427 batches | ms/batch 32.39 | loss 349.11\n",
      "| epoch  18 |  300/ 427 batches | ms/batch 32.29 | loss 341.59\n",
      "| epoch  18 |  400/ 427 batches | ms/batch 32.05 | loss 340.59\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  19 |  100/ 427 batches | ms/batch 32.61 | loss 342.76\n",
      "| epoch  19 |  200/ 427 batches | ms/batch 32.25 | loss 344.59\n",
      "| epoch  19 |  300/ 427 batches | ms/batch 32.21 | loss 346.64\n",
      "| epoch  19 |  400/ 427 batches | ms/batch 32.34 | loss 345.31\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  20 |  100/ 427 batches | ms/batch 32.87 | loss 343.67\n",
      "| epoch  20 |  200/ 427 batches | ms/batch 32.37 | loss 350.44\n",
      "| epoch  20 |  300/ 427 batches | ms/batch 32.31 | loss 345.30\n",
      "| epoch  20 |  400/ 427 batches | ms/batch 32.35 | loss 341.84\n",
      "Training took 13.81 seconds.\n",
      "Training phase...\n",
      "| epoch  21 |  100/ 427 batches | ms/batch 32.58 | loss 346.91\n",
      "| epoch  21 |  200/ 427 batches | ms/batch 32.17 | loss 345.39\n",
      "| epoch  21 |  300/ 427 batches | ms/batch 32.27 | loss 338.70\n",
      "| epoch  21 |  400/ 427 batches | ms/batch 32.20 | loss 346.16\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  22 |  100/ 427 batches | ms/batch 32.51 | loss 348.48\n",
      "| epoch  22 |  200/ 427 batches | ms/batch 32.27 | loss 344.06\n",
      "| epoch  22 |  300/ 427 batches | ms/batch 32.39 | loss 342.71\n",
      "| epoch  22 |  400/ 427 batches | ms/batch 32.00 | loss 341.44\n",
      "Training took 13.73 seconds.\n",
      "Training phase...\n",
      "| epoch  23 |  100/ 427 batches | ms/batch 32.68 | loss 344.08\n",
      "| epoch  23 |  200/ 427 batches | ms/batch 32.43 | loss 343.35\n",
      "| epoch  23 |  300/ 427 batches | ms/batch 32.58 | loss 346.22\n",
      "| epoch  23 |  400/ 427 batches | ms/batch 32.37 | loss 342.56\n",
      "Training took 13.82 seconds.\n",
      "Training phase...\n",
      "| epoch  24 |  100/ 427 batches | ms/batch 32.43 | loss 343.03\n",
      "| epoch  24 |  200/ 427 batches | ms/batch 32.18 | loss 343.52\n",
      "| epoch  24 |  300/ 427 batches | ms/batch 32.35 | loss 344.14\n",
      "| epoch  24 |  400/ 427 batches | ms/batch 32.19 | loss 345.13\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  25 |  100/ 427 batches | ms/batch 32.77 | loss 342.95\n",
      "| epoch  25 |  200/ 427 batches | ms/batch 32.32 | loss 343.73\n",
      "| epoch  25 |  300/ 427 batches | ms/batch 32.44 | loss 344.08\n",
      "| epoch  25 |  400/ 427 batches | ms/batch 32.39 | loss 344.35\n",
      "Training took 13.81 seconds.\n",
      "Training phase...\n",
      "| epoch  26 |  100/ 427 batches | ms/batch 32.46 | loss 343.74\n",
      "| epoch  26 |  200/ 427 batches | ms/batch 32.21 | loss 347.69\n",
      "| epoch  26 |  300/ 427 batches | ms/batch 32.16 | loss 343.74\n",
      "| epoch  26 |  400/ 427 batches | ms/batch 32.17 | loss 337.50\n",
      "Training took 13.73 seconds.\n",
      "Training phase...\n",
      "| epoch  27 |  100/ 427 batches | ms/batch 32.57 | loss 345.72\n",
      "| epoch  27 |  200/ 427 batches | ms/batch 32.28 | loss 341.12\n",
      "| epoch  27 |  300/ 427 batches | ms/batch 31.95 | loss 346.05\n",
      "| epoch  27 |  400/ 427 batches | ms/batch 31.99 | loss 341.87\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  28 |  100/ 427 batches | ms/batch 32.36 | loss 344.95\n",
      "| epoch  28 |  200/ 427 batches | ms/batch 32.11 | loss 343.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  28 |  300/ 427 batches | ms/batch 32.10 | loss 342.78\n",
      "| epoch  28 |  400/ 427 batches | ms/batch 32.31 | loss 342.64\n",
      "Training took 13.72 seconds.\n",
      "Training phase...\n",
      "| epoch  29 |  100/ 427 batches | ms/batch 32.65 | loss 345.61\n",
      "| epoch  29 |  200/ 427 batches | ms/batch 32.52 | loss 345.23\n",
      "| epoch  29 |  300/ 427 batches | ms/batch 32.29 | loss 342.02\n",
      "| epoch  29 |  400/ 427 batches | ms/batch 32.24 | loss 343.16\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  30 |  100/ 427 batches | ms/batch 32.44 | loss 346.85\n",
      "| epoch  30 |  200/ 427 batches | ms/batch 32.15 | loss 343.18\n",
      "| epoch  30 |  300/ 427 batches | ms/batch 32.22 | loss 338.45\n",
      "| epoch  30 |  400/ 427 batches | ms/batch 32.26 | loss 344.49\n",
      "Training took 13.73 seconds.\n",
      "Training phase...\n",
      "| epoch  31 |  100/ 427 batches | ms/batch 32.52 | loss 347.44\n",
      "| epoch  31 |  200/ 427 batches | ms/batch 32.28 | loss 340.73\n",
      "| epoch  31 |  300/ 427 batches | ms/batch 32.24 | loss 339.32\n",
      "| epoch  31 |  400/ 427 batches | ms/batch 32.44 | loss 343.25\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  32 |  100/ 427 batches | ms/batch 32.51 | loss 345.64\n",
      "| epoch  32 |  200/ 427 batches | ms/batch 32.17 | loss 344.88\n",
      "| epoch  32 |  300/ 427 batches | ms/batch 31.82 | loss 342.15\n",
      "| epoch  32 |  400/ 427 batches | ms/batch 32.24 | loss 340.64\n",
      "Training took 13.7 seconds.\n",
      "Training phase...\n",
      "| epoch  33 |  100/ 427 batches | ms/batch 32.57 | loss 344.93\n",
      "| epoch  33 |  200/ 427 batches | ms/batch 31.95 | loss 340.11\n",
      "| epoch  33 |  300/ 427 batches | ms/batch 31.97 | loss 342.00\n",
      "| epoch  33 |  400/ 427 batches | ms/batch 32.58 | loss 343.18\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  34 |  100/ 427 batches | ms/batch 32.66 | loss 343.55\n",
      "| epoch  34 |  200/ 427 batches | ms/batch 32.11 | loss 344.77\n",
      "| epoch  34 |  300/ 427 batches | ms/batch 32.13 | loss 340.50\n",
      "| epoch  34 |  400/ 427 batches | ms/batch 32.60 | loss 342.55\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  35 |  100/ 427 batches | ms/batch 32.61 | loss 347.99\n",
      "| epoch  35 |  200/ 427 batches | ms/batch 32.33 | loss 340.27\n",
      "| epoch  35 |  300/ 427 batches | ms/batch 32.36 | loss 346.00\n",
      "| epoch  35 |  400/ 427 batches | ms/batch 32.26 | loss 336.78\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  36 |  100/ 427 batches | ms/batch 32.35 | loss 345.04\n",
      "| epoch  36 |  200/ 427 batches | ms/batch 32.37 | loss 345.86\n",
      "| epoch  36 |  300/ 427 batches | ms/batch 32.49 | loss 339.46\n",
      "| epoch  36 |  400/ 427 batches | ms/batch 32.26 | loss 339.85\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  37 |  100/ 427 batches | ms/batch 32.59 | loss 343.16\n",
      "| epoch  37 |  200/ 427 batches | ms/batch 32.05 | loss 347.54\n",
      "| epoch  37 |  300/ 427 batches | ms/batch 32.06 | loss 337.82\n",
      "| epoch  37 |  400/ 427 batches | ms/batch 31.97 | loss 342.60\n",
      "Training took 13.69 seconds.\n",
      "Training phase...\n",
      "| epoch  38 |  100/ 427 batches | ms/batch 32.63 | loss 345.04\n",
      "| epoch  38 |  200/ 427 batches | ms/batch 32.05 | loss 342.28\n",
      "| epoch  38 |  300/ 427 batches | ms/batch 32.19 | loss 342.55\n",
      "| epoch  38 |  400/ 427 batches | ms/batch 32.32 | loss 341.24\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  39 |  100/ 427 batches | ms/batch 32.38 | loss 339.36\n",
      "| epoch  39 |  200/ 427 batches | ms/batch 32.16 | loss 342.29\n",
      "| epoch  39 |  300/ 427 batches | ms/batch 32.13 | loss 339.33\n",
      "| epoch  39 |  400/ 427 batches | ms/batch 32.12 | loss 347.09\n",
      "Training took 13.7 seconds.\n",
      "Training phase...\n",
      "| epoch  40 |  100/ 427 batches | ms/batch 32.62 | loss 349.23\n",
      "| epoch  40 |  200/ 427 batches | ms/batch 32.18 | loss 332.70\n",
      "| epoch  40 |  300/ 427 batches | ms/batch 32.29 | loss 342.81\n",
      "| epoch  40 |  400/ 427 batches | ms/batch 32.56 | loss 345.76\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  41 |  100/ 427 batches | ms/batch 32.67 | loss 342.14\n",
      "| epoch  41 |  200/ 427 batches | ms/batch 32.08 | loss 340.29\n",
      "| epoch  41 |  300/ 427 batches | ms/batch 32.43 | loss 341.20\n",
      "| epoch  41 |  400/ 427 batches | ms/batch 32.31 | loss 343.19\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  42 |  100/ 427 batches | ms/batch 32.81 | loss 344.03\n",
      "| epoch  42 |  200/ 427 batches | ms/batch 32.53 | loss 344.48\n",
      "| epoch  42 |  300/ 427 batches | ms/batch 32.37 | loss 336.43\n",
      "| epoch  42 |  400/ 427 batches | ms/batch 32.32 | loss 344.21\n",
      "Training took 13.83 seconds.\n",
      "Training phase...\n",
      "| epoch  43 |  100/ 427 batches | ms/batch 32.70 | loss 348.02\n",
      "| epoch  43 |  200/ 427 batches | ms/batch 32.25 | loss 339.42\n",
      "| epoch  43 |  300/ 427 batches | ms/batch 32.38 | loss 336.51\n",
      "| epoch  43 |  400/ 427 batches | ms/batch 32.18 | loss 345.20\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  44 |  100/ 427 batches | ms/batch 32.54 | loss 345.00\n",
      "| epoch  44 |  200/ 427 batches | ms/batch 32.24 | loss 341.17\n",
      "| epoch  44 |  300/ 427 batches | ms/batch 32.30 | loss 337.04\n",
      "| epoch  44 |  400/ 427 batches | ms/batch 32.39 | loss 342.09\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  45 |  100/ 427 batches | ms/batch 32.75 | loss 344.89\n",
      "| epoch  45 |  200/ 427 batches | ms/batch 32.79 | loss 342.14\n",
      "| epoch  45 |  300/ 427 batches | ms/batch 32.18 | loss 346.74\n",
      "| epoch  45 |  400/ 427 batches | ms/batch 32.43 | loss 337.84\n",
      "Training took 13.84 seconds.\n",
      "Training phase...\n",
      "| epoch  46 |  100/ 427 batches | ms/batch 32.56 | loss 343.49\n",
      "| epoch  46 |  200/ 427 batches | ms/batch 32.73 | loss 341.09\n",
      "| epoch  46 |  300/ 427 batches | ms/batch 32.22 | loss 342.13\n",
      "| epoch  46 |  400/ 427 batches | ms/batch 32.77 | loss 344.35\n",
      "Training took 13.86 seconds.\n",
      "Training phase...\n",
      "| epoch  47 |  100/ 427 batches | ms/batch 32.77 | loss 344.56\n",
      "| epoch  47 |  200/ 427 batches | ms/batch 32.40 | loss 337.28\n",
      "| epoch  47 |  300/ 427 batches | ms/batch 31.93 | loss 343.97\n",
      "| epoch  47 |  400/ 427 batches | ms/batch 32.19 | loss 341.47\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  48 |  100/ 427 batches | ms/batch 32.75 | loss 341.59\n",
      "| epoch  48 |  200/ 427 batches | ms/batch 32.21 | loss 340.90\n",
      "| epoch  48 |  300/ 427 batches | ms/batch 32.03 | loss 345.22\n",
      "| epoch  48 |  400/ 427 batches | ms/batch 32.34 | loss 339.94\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch  49 |  100/ 427 batches | ms/batch 32.78 | loss 346.87\n",
      "| epoch  49 |  200/ 427 batches | ms/batch 32.56 | loss 337.29\n",
      "| epoch  49 |  300/ 427 batches | ms/batch 32.23 | loss 343.39\n",
      "| epoch  49 |  400/ 427 batches | ms/batch 32.42 | loss 341.60\n",
      "Training took 13.82 seconds.\n",
      "Training phase...\n",
      "| epoch  50 |  100/ 427 batches | ms/batch 32.51 | loss 344.61\n",
      "| epoch  50 |  200/ 427 batches | ms/batch 32.19 | loss 341.13\n",
      "| epoch  50 |  300/ 427 batches | ms/batch 32.11 | loss 342.27\n",
      "| epoch  50 |  400/ 427 batches | ms/batch 32.20 | loss 340.95\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  51 |  100/ 427 batches | ms/batch 32.77 | loss 342.67\n",
      "| epoch  51 |  200/ 427 batches | ms/batch 32.17 | loss 342.11\n",
      "| epoch  51 |  300/ 427 batches | ms/batch 32.31 | loss 342.13\n",
      "| epoch  51 |  400/ 427 batches | ms/batch 32.40 | loss 343.52\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  52 |  100/ 427 batches | ms/batch 32.47 | loss 343.84\n",
      "| epoch  52 |  200/ 427 batches | ms/batch 32.14 | loss 339.87\n",
      "| epoch  52 |  300/ 427 batches | ms/batch 31.97 | loss 345.10\n",
      "| epoch  52 |  400/ 427 batches | ms/batch 32.26 | loss 338.52\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  53 |  100/ 427 batches | ms/batch 32.81 | loss 347.04\n",
      "| epoch  53 |  200/ 427 batches | ms/batch 32.40 | loss 337.89\n",
      "| epoch  53 |  300/ 427 batches | ms/batch 32.37 | loss 344.54\n",
      "| epoch  53 |  400/ 427 batches | ms/batch 32.28 | loss 339.03\n",
      "Training took 13.81 seconds.\n",
      "Training phase...\n",
      "| epoch  54 |  100/ 427 batches | ms/batch 32.71 | loss 347.54\n",
      "| epoch  54 |  200/ 427 batches | ms/batch 32.27 | loss 337.29\n",
      "| epoch  54 |  300/ 427 batches | ms/batch 32.22 | loss 339.14\n",
      "| epoch  54 |  400/ 427 batches | ms/batch 32.16 | loss 342.40\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch  55 |  100/ 427 batches | ms/batch 32.49 | loss 348.89\n",
      "| epoch  55 |  200/ 427 batches | ms/batch 32.27 | loss 341.62\n",
      "| epoch  55 |  300/ 427 batches | ms/batch 32.56 | loss 340.06\n",
      "| epoch  55 |  400/ 427 batches | ms/batch 32.74 | loss 336.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 13.84 seconds.\n",
      "Training phase...\n",
      "| epoch  56 |  100/ 427 batches | ms/batch 32.52 | loss 341.93\n",
      "| epoch  56 |  200/ 427 batches | ms/batch 32.30 | loss 339.42\n",
      "| epoch  56 |  300/ 427 batches | ms/batch 32.13 | loss 344.25\n",
      "| epoch  56 |  400/ 427 batches | ms/batch 32.23 | loss 340.69\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  57 |  100/ 427 batches | ms/batch 32.85 | loss 342.76\n",
      "| epoch  57 |  200/ 427 batches | ms/batch 32.46 | loss 342.76\n",
      "| epoch  57 |  300/ 427 batches | ms/batch 32.08 | loss 340.90\n",
      "| epoch  57 |  400/ 427 batches | ms/batch 32.24 | loss 340.29\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  58 |  100/ 427 batches | ms/batch 32.70 | loss 344.31\n",
      "| epoch  58 |  200/ 427 batches | ms/batch 32.39 | loss 339.19\n",
      "| epoch  58 |  300/ 427 batches | ms/batch 32.17 | loss 347.38\n",
      "| epoch  58 |  400/ 427 batches | ms/batch 32.44 | loss 336.99\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  59 |  100/ 427 batches | ms/batch 32.53 | loss 343.11\n",
      "| epoch  59 |  200/ 427 batches | ms/batch 32.22 | loss 341.09\n",
      "| epoch  59 |  300/ 427 batches | ms/batch 32.49 | loss 338.73\n",
      "| epoch  59 |  400/ 427 batches | ms/batch 32.52 | loss 342.06\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  60 |  100/ 427 batches | ms/batch 32.51 | loss 345.02\n",
      "| epoch  60 |  200/ 427 batches | ms/batch 32.03 | loss 339.26\n",
      "| epoch  60 |  300/ 427 batches | ms/batch 32.21 | loss 341.69\n",
      "| epoch  60 |  400/ 427 batches | ms/batch 32.30 | loss 341.68\n",
      "Training took 13.72 seconds.\n",
      "Training phase...\n",
      "| epoch  61 |  100/ 427 batches | ms/batch 32.56 | loss 345.37\n",
      "| epoch  61 |  200/ 427 batches | ms/batch 32.34 | loss 341.92\n",
      "| epoch  61 |  300/ 427 batches | ms/batch 32.33 | loss 338.19\n",
      "| epoch  61 |  400/ 427 batches | ms/batch 32.42 | loss 339.29\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  62 |  100/ 427 batches | ms/batch 32.77 | loss 345.92\n",
      "| epoch  62 |  200/ 427 batches | ms/batch 32.20 | loss 340.63\n",
      "| epoch  62 |  300/ 427 batches | ms/batch 32.14 | loss 337.73\n",
      "| epoch  62 |  400/ 427 batches | ms/batch 32.37 | loss 342.20\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  63 |  100/ 427 batches | ms/batch 32.69 | loss 346.93\n",
      "| epoch  63 |  200/ 427 batches | ms/batch 32.37 | loss 339.87\n",
      "| epoch  63 |  300/ 427 batches | ms/batch 32.30 | loss 340.32\n",
      "| epoch  63 |  400/ 427 batches | ms/batch 32.18 | loss 339.96\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  64 |  100/ 427 batches | ms/batch 32.62 | loss 343.75\n",
      "| epoch  64 |  200/ 427 batches | ms/batch 32.56 | loss 335.07\n",
      "| epoch  64 |  300/ 427 batches | ms/batch 32.35 | loss 343.32\n",
      "| epoch  64 |  400/ 427 batches | ms/batch 32.13 | loss 343.44\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  65 |  100/ 427 batches | ms/batch 32.64 | loss 345.92\n",
      "| epoch  65 |  200/ 427 batches | ms/batch 32.24 | loss 340.03\n",
      "| epoch  65 |  300/ 427 batches | ms/batch 32.21 | loss 333.77\n",
      "| epoch  65 |  400/ 427 batches | ms/batch 32.25 | loss 344.87\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch  66 |  100/ 427 batches | ms/batch 32.49 | loss 341.42\n",
      "| epoch  66 |  200/ 427 batches | ms/batch 32.25 | loss 341.39\n",
      "| epoch  66 |  300/ 427 batches | ms/batch 32.13 | loss 342.29\n",
      "| epoch  66 |  400/ 427 batches | ms/batch 32.52 | loss 340.57\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch  67 |  100/ 427 batches | ms/batch 32.85 | loss 343.37\n",
      "| epoch  67 |  200/ 427 batches | ms/batch 32.28 | loss 341.37\n",
      "| epoch  67 |  300/ 427 batches | ms/batch 31.88 | loss 344.33\n",
      "| epoch  67 |  400/ 427 batches | ms/batch 32.17 | loss 338.33\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  68 |  100/ 427 batches | ms/batch 32.67 | loss 340.74\n",
      "| epoch  68 |  200/ 427 batches | ms/batch 32.29 | loss 340.56\n",
      "| epoch  68 |  300/ 427 batches | ms/batch 32.51 | loss 342.50\n",
      "| epoch  68 |  400/ 427 batches | ms/batch 32.26 | loss 341.71\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  69 |  100/ 427 batches | ms/batch 32.61 | loss 341.22\n",
      "| epoch  69 |  200/ 427 batches | ms/batch 32.13 | loss 345.58\n",
      "| epoch  69 |  300/ 427 batches | ms/batch 32.29 | loss 338.43\n",
      "| epoch  69 |  400/ 427 batches | ms/batch 32.40 | loss 339.54\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  70 |  100/ 427 batches | ms/batch 32.61 | loss 343.77\n",
      "| epoch  70 |  200/ 427 batches | ms/batch 32.26 | loss 339.20\n",
      "| epoch  70 |  300/ 427 batches | ms/batch 32.14 | loss 341.18\n",
      "| epoch  70 |  400/ 427 batches | ms/batch 32.49 | loss 341.31\n",
      "Training took 13.77 seconds.\n",
      "Training phase...\n",
      "| epoch  71 |  100/ 427 batches | ms/batch 32.67 | loss 343.19\n",
      "| epoch  71 |  200/ 427 batches | ms/batch 32.32 | loss 339.16\n",
      "| epoch  71 |  300/ 427 batches | ms/batch 32.49 | loss 341.94\n",
      "| epoch  71 |  400/ 427 batches | ms/batch 32.49 | loss 342.19\n",
      "Training took 13.82 seconds.\n",
      "Training phase...\n",
      "| epoch  72 |  100/ 427 batches | ms/batch 32.54 | loss 346.91\n",
      "| epoch  72 |  200/ 427 batches | ms/batch 32.44 | loss 340.75\n",
      "| epoch  72 |  300/ 427 batches | ms/batch 32.22 | loss 339.30\n",
      "| epoch  72 |  400/ 427 batches | ms/batch 32.22 | loss 337.90\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  73 |  100/ 427 batches | ms/batch 32.77 | loss 343.95\n",
      "| epoch  73 |  200/ 427 batches | ms/batch 32.31 | loss 339.98\n",
      "| epoch  73 |  300/ 427 batches | ms/batch 32.11 | loss 341.34\n",
      "| epoch  73 |  400/ 427 batches | ms/batch 32.02 | loss 338.47\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  74 |  100/ 427 batches | ms/batch 32.49 | loss 340.85\n",
      "| epoch  74 |  200/ 427 batches | ms/batch 32.14 | loss 337.66\n",
      "| epoch  74 |  300/ 427 batches | ms/batch 32.24 | loss 341.71\n",
      "| epoch  74 |  400/ 427 batches | ms/batch 32.20 | loss 342.87\n",
      "Training took 13.73 seconds.\n",
      "Training phase...\n",
      "| epoch  75 |  100/ 427 batches | ms/batch 32.36 | loss 341.27\n",
      "| epoch  75 |  200/ 427 batches | ms/batch 32.14 | loss 337.80\n",
      "| epoch  75 |  300/ 427 batches | ms/batch 32.33 | loss 339.76\n",
      "| epoch  75 |  400/ 427 batches | ms/batch 32.31 | loss 344.22\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  76 |  100/ 427 batches | ms/batch 32.63 | loss 342.98\n",
      "| epoch  76 |  200/ 427 batches | ms/batch 32.29 | loss 341.60\n",
      "| epoch  76 |  300/ 427 batches | ms/batch 32.06 | loss 342.80\n",
      "| epoch  76 |  400/ 427 batches | ms/batch 32.36 | loss 336.96\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch  77 |  100/ 427 batches | ms/batch 32.67 | loss 344.93\n",
      "| epoch  77 |  200/ 427 batches | ms/batch 32.53 | loss 337.05\n",
      "| epoch  77 |  300/ 427 batches | ms/batch 32.52 | loss 339.50\n",
      "| epoch  77 |  400/ 427 batches | ms/batch 31.92 | loss 342.74\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  78 |  100/ 427 batches | ms/batch 32.62 | loss 342.63\n",
      "| epoch  78 |  200/ 427 batches | ms/batch 32.51 | loss 339.13\n",
      "| epoch  78 |  300/ 427 batches | ms/batch 32.36 | loss 340.40\n",
      "| epoch  78 |  400/ 427 batches | ms/batch 32.22 | loss 342.49\n",
      "Training took 13.79 seconds.\n",
      "Training phase...\n",
      "| epoch  79 |  100/ 427 batches | ms/batch 32.35 | loss 343.56\n",
      "| epoch  79 |  200/ 427 batches | ms/batch 32.21 | loss 341.72\n",
      "| epoch  79 |  300/ 427 batches | ms/batch 32.36 | loss 339.09\n",
      "| epoch  79 |  400/ 427 batches | ms/batch 32.27 | loss 341.74\n",
      "Training took 13.74 seconds.\n",
      "Training phase...\n",
      "| epoch  80 |  100/ 427 batches | ms/batch 32.97 | loss 347.08\n",
      "| epoch  80 |  200/ 427 batches | ms/batch 32.39 | loss 342.19\n",
      "| epoch  80 |  300/ 427 batches | ms/batch 32.37 | loss 339.20\n",
      "| epoch  80 |  400/ 427 batches | ms/batch 32.39 | loss 336.67\n",
      "Training took 13.84 seconds.\n",
      "Training phase...\n",
      "| epoch  81 |  100/ 427 batches | ms/batch 32.44 | loss 343.30\n",
      "| epoch  81 |  200/ 427 batches | ms/batch 32.33 | loss 341.29\n",
      "| epoch  81 |  300/ 427 batches | ms/batch 32.58 | loss 337.58\n",
      "| epoch  81 |  400/ 427 batches | ms/batch 32.41 | loss 340.74\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  82 |  100/ 427 batches | ms/batch 32.64 | loss 345.27\n",
      "| epoch  82 |  200/ 427 batches | ms/batch 32.12 | loss 340.98\n",
      "| epoch  82 |  300/ 427 batches | ms/batch 31.91 | loss 338.60\n",
      "| epoch  82 |  400/ 427 batches | ms/batch 32.19 | loss 338.04\n",
      "Training took 13.7 seconds.\n",
      "Training phase...\n",
      "| epoch  83 |  100/ 427 batches | ms/batch 32.82 | loss 343.58\n",
      "| epoch  83 |  200/ 427 batches | ms/batch 31.86 | loss 337.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  83 |  300/ 427 batches | ms/batch 32.14 | loss 341.56\n",
      "| epoch  83 |  400/ 427 batches | ms/batch 31.92 | loss 339.72\n",
      "Training took 13.69 seconds.\n",
      "Training phase...\n",
      "| epoch  84 |  100/ 427 batches | ms/batch 32.32 | loss 340.01\n",
      "| epoch  84 |  200/ 427 batches | ms/batch 32.09 | loss 342.35\n",
      "| epoch  84 |  300/ 427 batches | ms/batch 32.03 | loss 343.13\n",
      "| epoch  84 |  400/ 427 batches | ms/batch 32.06 | loss 339.65\n",
      "Training took 13.67 seconds.\n",
      "Training phase...\n",
      "| epoch  85 |  100/ 427 batches | ms/batch 32.38 | loss 339.76\n",
      "| epoch  85 |  200/ 427 batches | ms/batch 32.16 | loss 343.57\n",
      "| epoch  85 |  300/ 427 batches | ms/batch 31.99 | loss 338.69\n",
      "| epoch  85 |  400/ 427 batches | ms/batch 32.18 | loss 338.27\n",
      "Training took 13.68 seconds.\n",
      "Training phase...\n",
      "| epoch  86 |  100/ 427 batches | ms/batch 32.84 | loss 341.19\n",
      "| epoch  86 |  200/ 427 batches | ms/batch 32.27 | loss 343.61\n",
      "| epoch  86 |  300/ 427 batches | ms/batch 32.00 | loss 341.55\n",
      "| epoch  86 |  400/ 427 batches | ms/batch 32.18 | loss 337.84\n",
      "Training took 13.75 seconds.\n",
      "Training phase...\n",
      "| epoch  87 |  100/ 427 batches | ms/batch 32.41 | loss 339.67\n",
      "| epoch  87 |  200/ 427 batches | ms/batch 31.69 | loss 338.12\n",
      "| epoch  87 |  300/ 427 batches | ms/batch 31.86 | loss 341.85\n",
      "| epoch  87 |  400/ 427 batches | ms/batch 32.10 | loss 342.21\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  88 |  100/ 427 batches | ms/batch 32.40 | loss 344.39\n",
      "| epoch  88 |  200/ 427 batches | ms/batch 31.96 | loss 340.77\n",
      "| epoch  88 |  300/ 427 batches | ms/batch 32.24 | loss 340.07\n",
      "| epoch  88 |  400/ 427 batches | ms/batch 32.10 | loss 334.84\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  89 |  100/ 427 batches | ms/batch 32.39 | loss 344.26\n",
      "| epoch  89 |  200/ 427 batches | ms/batch 31.93 | loss 337.28\n",
      "| epoch  89 |  300/ 427 batches | ms/batch 32.12 | loss 339.21\n",
      "| epoch  89 |  400/ 427 batches | ms/batch 32.14 | loss 341.09\n",
      "Training took 13.68 seconds.\n",
      "Training phase...\n",
      "| epoch  90 |  100/ 427 batches | ms/batch 32.51 | loss 345.32\n",
      "| epoch  90 |  200/ 427 batches | ms/batch 32.17 | loss 338.93\n",
      "| epoch  90 |  300/ 427 batches | ms/batch 31.99 | loss 336.61\n",
      "| epoch  90 |  400/ 427 batches | ms/batch 31.98 | loss 342.20\n",
      "Training took 13.69 seconds.\n",
      "Training phase...\n",
      "| epoch  91 |  100/ 427 batches | ms/batch 32.53 | loss 344.40\n",
      "| epoch  91 |  200/ 427 batches | ms/batch 32.09 | loss 337.71\n",
      "| epoch  91 |  300/ 427 batches | ms/batch 32.01 | loss 342.54\n",
      "| epoch  91 |  400/ 427 batches | ms/batch 32.11 | loss 338.99\n",
      "Training took 13.7 seconds.\n",
      "Training phase...\n",
      "| epoch  92 |  100/ 427 batches | ms/batch 32.36 | loss 338.24\n",
      "| epoch  92 |  200/ 427 batches | ms/batch 31.82 | loss 341.23\n",
      "| epoch  92 |  300/ 427 batches | ms/batch 31.97 | loss 341.91\n",
      "| epoch  92 |  400/ 427 batches | ms/batch 32.02 | loss 340.97\n",
      "Training took 13.64 seconds.\n",
      "Training phase...\n",
      "| epoch  93 |  100/ 427 batches | ms/batch 32.47 | loss 347.28\n",
      "| epoch  93 |  200/ 427 batches | ms/batch 32.21 | loss 335.44\n",
      "| epoch  93 |  300/ 427 batches | ms/batch 31.97 | loss 339.66\n",
      "| epoch  93 |  400/ 427 batches | ms/batch 32.01 | loss 341.30\n",
      "Training took 13.69 seconds.\n",
      "Training phase...\n",
      "| epoch  94 |  100/ 427 batches | ms/batch 32.47 | loss 343.41\n",
      "| epoch  94 |  200/ 427 batches | ms/batch 32.14 | loss 340.45\n",
      "| epoch  94 |  300/ 427 batches | ms/batch 32.11 | loss 338.72\n",
      "| epoch  94 |  400/ 427 batches | ms/batch 32.13 | loss 340.50\n",
      "Training took 13.7 seconds.\n",
      "Training phase...\n",
      "| epoch  95 |  100/ 427 batches | ms/batch 32.41 | loss 342.12\n",
      "| epoch  95 |  200/ 427 batches | ms/batch 32.24 | loss 340.11\n",
      "| epoch  95 |  300/ 427 batches | ms/batch 32.01 | loss 340.85\n",
      "| epoch  95 |  400/ 427 batches | ms/batch 32.06 | loss 339.43\n",
      "Training took 13.7 seconds.\n",
      "Training phase...\n",
      "| epoch  96 |  100/ 427 batches | ms/batch 32.44 | loss 344.05\n",
      "| epoch  96 |  200/ 427 batches | ms/batch 32.20 | loss 338.23\n",
      "| epoch  96 |  300/ 427 batches | ms/batch 32.24 | loss 338.78\n",
      "| epoch  96 |  400/ 427 batches | ms/batch 32.15 | loss 341.65\n",
      "Training took 13.72 seconds.\n",
      "Training phase...\n",
      "| epoch  97 |  100/ 427 batches | ms/batch 32.45 | loss 336.15\n",
      "| epoch  97 |  200/ 427 batches | ms/batch 31.79 | loss 342.98\n",
      "| epoch  97 |  300/ 427 batches | ms/batch 32.08 | loss 341.86\n",
      "| epoch  97 |  400/ 427 batches | ms/batch 32.10 | loss 339.43\n",
      "Training took 13.67 seconds.\n",
      "Training phase...\n",
      "| epoch  98 |  100/ 427 batches | ms/batch 32.50 | loss 341.99\n",
      "| epoch  98 |  200/ 427 batches | ms/batch 31.93 | loss 336.74\n",
      "| epoch  98 |  300/ 427 batches | ms/batch 32.19 | loss 335.70\n",
      "| epoch  98 |  400/ 427 batches | ms/batch 32.01 | loss 346.90\n",
      "Training took 13.68 seconds.\n",
      "Training phase...\n",
      "| epoch  99 |  100/ 427 batches | ms/batch 32.56 | loss 339.04\n",
      "| epoch  99 |  200/ 427 batches | ms/batch 32.48 | loss 343.81\n",
      "| epoch  99 |  300/ 427 batches | ms/batch 31.94 | loss 339.99\n",
      "| epoch  99 |  400/ 427 batches | ms/batch 32.03 | loss 339.94\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch 100 |  100/ 427 batches | ms/batch 32.25 | loss 340.64\n",
      "| epoch 100 |  200/ 427 batches | ms/batch 32.16 | loss 338.14\n",
      "| epoch 100 |  300/ 427 batches | ms/batch 32.00 | loss 341.01\n",
      "| epoch 100 |  400/ 427 batches | ms/batch 32.22 | loss 341.93\n",
      "Training took 13.68 seconds.\n"
     ]
    }
   ],
   "source": [
    "model = TrainableMultVAE(base_params[\"encoder_dims\"], base_params[\"decoder_dims\"], base_params[\"dropout\"])\n",
    "optimizer = optim.Adam(model.parameters(), **train_params[\"optimizer_kwargs\"])\n",
    "\n",
    "model.fit(train_data, optimizer, criterion, val_data=None, n_epochs=train_params[\"n_epochs\"],\n",
    "          k=train_params[\"k\"], beta=train_params[\"beta\"])\n",
    "\n",
    "ndcg, err, hr = model.predict_metrics(train_data, validation_data)\n",
    "\n",
    "res = {}\n",
    "\n",
    "res[\"NDCG10\"] = ndcg\n",
    "res[\"ERR10\"] = err\n",
    "res[\"HR10\"] = hr\n",
    "res[\"dataset\"] = \"explicit_binarized\"\n",
    "\n",
    "with open(tmp_savepath+\"vae_explicit_binarized\", \"wb\") as handle:\n",
    "    pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477f9f0",
   "metadata": {},
   "source": [
    "### SLIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "081132bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning all 2265 vectors took 1.44 minutes.\n",
      "In W matrix we have 65679 nonzero elements (1.28%).\n",
      "Computing top-k list for each user...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e063649e89de4d4e9fc912ace3a043e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...evaluation...\n",
      "0.18763862360168257 0.153417942660178 0.29900810384657694\n"
     ]
    }
   ],
   "source": [
    "# set the parameters\n",
    "slim = ParallSynSLIM(l1_reg, l2_reg)\n",
    "\n",
    "# train the model\n",
    "slim.fit(train_data)\n",
    "\n",
    "# how many nonzero entires in W matirx\n",
    "proc = 100*slim.W.nnz/slim.W.shape[0]**2\n",
    "\n",
    "print(\"Computing top-k list for each user...\")\n",
    "# produce top k list for all users\n",
    "start = time.time()\n",
    "top_k_list = slim.calculate_top_k(train_data, ids_to_gs, ids_to_us, k=k)\n",
    "pred_time = time.time() - start\n",
    "\n",
    "print(\"...evaluation...\")\n",
    "ev = Evaluator(k=k, true=validation_list, predicted=top_k_list)\n",
    "ev.calculate_metrics()\n",
    "ngcg10, err10, hr10 = ev.ndcg, ev.err, ev.hr\n",
    "\n",
    "# save the obtained results\n",
    "res = {}\n",
    "res[\"ndcg10\"] = ngcg10\n",
    "res[\"err10\"] = err10\n",
    "res[\"hr10\"] = hr10\n",
    "res[\"W_zeros_percentage\"] = proc\n",
    "res[\"prediction_calc_time_seconds\"] = pred_time\n",
    "res[\"datset\"] = \"explicit_binarized\"\n",
    "\n",
    "\n",
    "print(ngcg10, err10, hr10)\n",
    "# save the obtained results\n",
    "with open(tmp_savepath_slim+\"SLIM_explicit_binarized\", \"wb\") as handle:\n",
    "    pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3492072a",
   "metadata": {},
   "source": [
    "## Binarized filtered explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ea45980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmarzec12/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Explicit dataset\n",
    "explicit_filtered = explicit[explicit.score > 6]\n",
    "explicit_filtered.score = 1\n",
    "row = [us_to_ids[us] for us in explicit_filtered.user_name]\n",
    "col = [gs_to_ids[g] for g in explicit_filtered.game_id]\n",
    "data = explicit_filtered.score\n",
    "\n",
    "train_data = scipy.sparse.coo_matrix((data, (row, col)), shape=(len(unique_users), len(unique_games))).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fe48e",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "278fd546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase...\n",
      "| epoch   1 |  100/ 427 batches | ms/batch 32.40 | loss 298.55\n",
      "| epoch   1 |  200/ 427 batches | ms/batch 31.78 | loss 283.94\n",
      "| epoch   1 |  300/ 427 batches | ms/batch 32.05 | loss 281.77\n",
      "| epoch   1 |  400/ 427 batches | ms/batch 31.74 | loss 277.16\n",
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch   2 |  100/ 427 batches | ms/batch 32.71 | loss 279.44\n",
      "| epoch   2 |  200/ 427 batches | ms/batch 31.85 | loss 272.13\n",
      "| epoch   2 |  300/ 427 batches | ms/batch 31.82 | loss 273.43\n",
      "| epoch   2 |  400/ 427 batches | ms/batch 31.78 | loss 271.45\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch   3 |  100/ 427 batches | ms/batch 32.04 | loss 273.47\n",
      "| epoch   3 |  200/ 427 batches | ms/batch 31.83 | loss 270.82\n",
      "| epoch   3 |  300/ 427 batches | ms/batch 31.84 | loss 272.64\n",
      "| epoch   3 |  400/ 427 batches | ms/batch 31.84 | loss 265.53\n",
      "Training took 13.57 seconds.\n",
      "Training phase...\n",
      "| epoch   4 |  100/ 427 batches | ms/batch 32.30 | loss 271.99\n",
      "| epoch   4 |  200/ 427 batches | ms/batch 32.05 | loss 267.93\n",
      "| epoch   4 |  300/ 427 batches | ms/batch 31.97 | loss 267.42\n",
      "| epoch   4 |  400/ 427 batches | ms/batch 32.01 | loss 264.56\n",
      "Training took 13.64 seconds.\n",
      "Training phase...\n",
      "| epoch   5 |  100/ 427 batches | ms/batch 32.12 | loss 268.02\n",
      "| epoch   5 |  200/ 427 batches | ms/batch 31.55 | loss 266.44\n",
      "| epoch   5 |  300/ 427 batches | ms/batch 31.70 | loss 267.28\n",
      "| epoch   5 |  400/ 427 batches | ms/batch 31.75 | loss 265.41\n",
      "Training took 13.53 seconds.\n",
      "Training phase...\n",
      "| epoch   6 |  100/ 427 batches | ms/batch 32.18 | loss 269.44\n",
      "| epoch   6 |  200/ 427 batches | ms/batch 31.86 | loss 262.83\n",
      "| epoch   6 |  300/ 427 batches | ms/batch 31.89 | loss 265.57\n",
      "| epoch   6 |  400/ 427 batches | ms/batch 32.01 | loss 263.86\n",
      "Training took 13.6 seconds.\n",
      "Training phase...\n",
      "| epoch   7 |  100/ 427 batches | ms/batch 32.14 | loss 264.97\n",
      "| epoch   7 |  200/ 427 batches | ms/batch 31.84 | loss 258.91\n",
      "| epoch   7 |  300/ 427 batches | ms/batch 31.87 | loss 266.26\n",
      "| epoch   7 |  400/ 427 batches | ms/batch 31.75 | loss 266.60\n",
      "Training took 13.58 seconds.\n",
      "Training phase...\n",
      "| epoch   8 |  100/ 427 batches | ms/batch 32.27 | loss 262.57\n",
      "| epoch   8 |  200/ 427 batches | ms/batch 31.88 | loss 262.96\n",
      "| epoch   8 |  300/ 427 batches | ms/batch 31.89 | loss 263.37\n",
      "| epoch   8 |  400/ 427 batches | ms/batch 31.85 | loss 263.57\n",
      "Training took 13.6 seconds.\n",
      "Training phase...\n",
      "| epoch   9 |  100/ 427 batches | ms/batch 32.21 | loss 262.66\n",
      "| epoch   9 |  200/ 427 batches | ms/batch 31.73 | loss 259.16\n",
      "| epoch   9 |  300/ 427 batches | ms/batch 31.82 | loss 265.27\n",
      "| epoch   9 |  400/ 427 batches | ms/batch 31.82 | loss 261.86\n",
      "Training took 13.57 seconds.\n",
      "Training phase...\n",
      "| epoch  10 |  100/ 427 batches | ms/batch 32.16 | loss 264.95\n",
      "| epoch  10 |  200/ 427 batches | ms/batch 31.47 | loss 262.55\n",
      "| epoch  10 |  300/ 427 batches | ms/batch 31.76 | loss 258.66\n",
      "| epoch  10 |  400/ 427 batches | ms/batch 31.90 | loss 261.96\n",
      "Training took 13.54 seconds.\n",
      "Training phase...\n",
      "| epoch  11 |  100/ 427 batches | ms/batch 32.10 | loss 260.44\n",
      "| epoch  11 |  200/ 427 batches | ms/batch 31.74 | loss 263.66\n",
      "| epoch  11 |  300/ 427 batches | ms/batch 31.64 | loss 261.74\n",
      "| epoch  11 |  400/ 427 batches | ms/batch 31.75 | loss 261.18\n",
      "Training took 13.53 seconds.\n",
      "Training phase...\n",
      "| epoch  12 |  100/ 427 batches | ms/batch 32.11 | loss 262.85\n",
      "| epoch  12 |  200/ 427 batches | ms/batch 31.82 | loss 261.18\n",
      "| epoch  12 |  300/ 427 batches | ms/batch 31.76 | loss 260.22\n",
      "| epoch  12 |  400/ 427 batches | ms/batch 31.68 | loss 260.82\n",
      "Training took 13.55 seconds.\n",
      "Training phase...\n",
      "| epoch  13 |  100/ 427 batches | ms/batch 32.28 | loss 260.80\n",
      "| epoch  13 |  200/ 427 batches | ms/batch 32.15 | loss 258.72\n",
      "| epoch  13 |  300/ 427 batches | ms/batch 32.09 | loss 262.18\n",
      "| epoch  13 |  400/ 427 batches | ms/batch 32.17 | loss 260.77\n",
      "Training took 13.69 seconds.\n",
      "Training phase...\n",
      "| epoch  14 |  100/ 427 batches | ms/batch 32.29 | loss 263.49\n",
      "| epoch  14 |  200/ 427 batches | ms/batch 31.95 | loss 259.78\n",
      "| epoch  14 |  300/ 427 batches | ms/batch 31.96 | loss 261.26\n",
      "| epoch  14 |  400/ 427 batches | ms/batch 32.18 | loss 259.89\n",
      "Training took 13.66 seconds.\n",
      "Training phase...\n",
      "| epoch  15 |  100/ 427 batches | ms/batch 32.47 | loss 261.57\n",
      "| epoch  15 |  200/ 427 batches | ms/batch 31.90 | loss 262.63\n",
      "| epoch  15 |  300/ 427 batches | ms/batch 31.49 | loss 257.41\n",
      "| epoch  15 |  400/ 427 batches | ms/batch 31.70 | loss 257.43\n",
      "Training took 13.57 seconds.\n",
      "Training phase...\n",
      "| epoch  16 |  100/ 427 batches | ms/batch 32.22 | loss 261.75\n",
      "| epoch  16 |  200/ 427 batches | ms/batch 31.98 | loss 260.60\n",
      "| epoch  16 |  300/ 427 batches | ms/batch 31.87 | loss 257.19\n",
      "| epoch  16 |  400/ 427 batches | ms/batch 31.95 | loss 259.86\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  17 |  100/ 427 batches | ms/batch 32.20 | loss 258.79\n",
      "| epoch  17 |  200/ 427 batches | ms/batch 31.83 | loss 259.10\n",
      "| epoch  17 |  300/ 427 batches | ms/batch 31.92 | loss 259.46\n",
      "| epoch  17 |  400/ 427 batches | ms/batch 31.96 | loss 262.41\n",
      "Training took 13.6 seconds.\n",
      "Training phase...\n",
      "| epoch  18 |  100/ 427 batches | ms/batch 32.11 | loss 260.70\n",
      "| epoch  18 |  200/ 427 batches | ms/batch 31.79 | loss 257.34\n",
      "| epoch  18 |  300/ 427 batches | ms/batch 31.96 | loss 257.82\n",
      "| epoch  18 |  400/ 427 batches | ms/batch 31.99 | loss 261.72\n",
      "Training took 13.59 seconds.\n",
      "Training phase...\n",
      "| epoch  19 |  100/ 427 batches | ms/batch 32.21 | loss 259.43\n",
      "| epoch  19 |  200/ 427 batches | ms/batch 31.87 | loss 261.64\n",
      "| epoch  19 |  300/ 427 batches | ms/batch 31.94 | loss 257.51\n",
      "| epoch  19 |  400/ 427 batches | ms/batch 31.98 | loss 258.88\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  20 |  100/ 427 batches | ms/batch 32.28 | loss 262.39\n",
      "| epoch  20 |  200/ 427 batches | ms/batch 32.11 | loss 256.84\n",
      "| epoch  20 |  300/ 427 batches | ms/batch 32.03 | loss 256.65\n",
      "| epoch  20 |  400/ 427 batches | ms/batch 31.79 | loss 260.64\n",
      "Training took 13.64 seconds.\n",
      "Training phase...\n",
      "| epoch  21 |  100/ 427 batches | ms/batch 32.23 | loss 260.27\n",
      "| epoch  21 |  200/ 427 batches | ms/batch 31.79 | loss 262.06\n",
      "| epoch  21 |  300/ 427 batches | ms/batch 31.84 | loss 258.04\n",
      "| epoch  21 |  400/ 427 batches | ms/batch 31.96 | loss 257.06\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  22 |  100/ 427 batches | ms/batch 32.39 | loss 260.50\n",
      "| epoch  22 |  200/ 427 batches | ms/batch 31.68 | loss 257.80\n",
      "| epoch  22 |  300/ 427 batches | ms/batch 31.82 | loss 258.03\n",
      "| epoch  22 |  400/ 427 batches | ms/batch 31.93 | loss 260.02\n",
      "Training took 13.6 seconds.\n",
      "Training phase...\n",
      "| epoch  23 |  100/ 427 batches | ms/batch 32.17 | loss 258.78\n",
      "| epoch  23 |  200/ 427 batches | ms/batch 31.73 | loss 257.79\n",
      "| epoch  23 |  300/ 427 batches | ms/batch 31.83 | loss 258.94\n",
      "| epoch  23 |  400/ 427 batches | ms/batch 31.82 | loss 258.06\n",
      "Training took 13.58 seconds.\n",
      "Training phase...\n",
      "| epoch  24 |  100/ 427 batches | ms/batch 32.66 | loss 259.95\n",
      "| epoch  24 |  200/ 427 batches | ms/batch 31.91 | loss 258.29\n",
      "| epoch  24 |  300/ 427 batches | ms/batch 31.72 | loss 258.02\n",
      "| epoch  24 |  400/ 427 batches | ms/batch 31.78 | loss 259.64\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  25 |  100/ 427 batches | ms/batch 32.12 | loss 257.39\n",
      "| epoch  25 |  200/ 427 batches | ms/batch 32.11 | loss 259.59\n",
      "| epoch  25 |  300/ 427 batches | ms/batch 31.93 | loss 259.96\n",
      "| epoch  25 |  400/ 427 batches | ms/batch 31.89 | loss 257.85\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  26 |  100/ 427 batches | ms/batch 32.22 | loss 258.95\n",
      "| epoch  26 |  200/ 427 batches | ms/batch 31.98 | loss 257.63\n",
      "| epoch  26 |  300/ 427 batches | ms/batch 31.90 | loss 258.64\n",
      "| epoch  26 |  400/ 427 batches | ms/batch 32.02 | loss 260.28\n",
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch  27 |  100/ 427 batches | ms/batch 32.20 | loss 258.85\n",
      "| epoch  27 |  200/ 427 batches | ms/batch 31.99 | loss 257.94\n",
      "| epoch  27 |  300/ 427 batches | ms/batch 31.73 | loss 257.83\n",
      "| epoch  27 |  400/ 427 batches | ms/batch 31.88 | loss 260.24\n",
      "Training took 13.59 seconds.\n",
      "Training phase...\n",
      "| epoch  28 |  100/ 427 batches | ms/batch 32.01 | loss 255.80\n",
      "| epoch  28 |  200/ 427 batches | ms/batch 32.10 | loss 259.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  28 |  300/ 427 batches | ms/batch 31.87 | loss 256.67\n",
      "| epoch  28 |  400/ 427 batches | ms/batch 31.85 | loss 260.09\n",
      "Training took 13.6 seconds.\n",
      "Training phase...\n",
      "| epoch  29 |  100/ 427 batches | ms/batch 32.12 | loss 260.74\n",
      "| epoch  29 |  200/ 427 batches | ms/batch 31.97 | loss 255.63\n",
      "| epoch  29 |  300/ 427 batches | ms/batch 31.95 | loss 257.05\n",
      "| epoch  29 |  400/ 427 batches | ms/batch 31.84 | loss 258.67\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  30 |  100/ 427 batches | ms/batch 32.32 | loss 260.10\n",
      "| epoch  30 |  200/ 427 batches | ms/batch 31.93 | loss 256.67\n",
      "| epoch  30 |  300/ 427 batches | ms/batch 31.75 | loss 259.13\n",
      "| epoch  30 |  400/ 427 batches | ms/batch 32.16 | loss 257.13\n",
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch  31 |  100/ 427 batches | ms/batch 32.36 | loss 262.49\n",
      "| epoch  31 |  200/ 427 batches | ms/batch 32.09 | loss 257.60\n",
      "| epoch  31 |  300/ 427 batches | ms/batch 31.86 | loss 251.22\n",
      "| epoch  31 |  400/ 427 batches | ms/batch 32.00 | loss 260.72\n",
      "Training took 13.65 seconds.\n",
      "Training phase...\n",
      "| epoch  32 |  100/ 427 batches | ms/batch 31.94 | loss 260.69\n",
      "| epoch  32 |  200/ 427 batches | ms/batch 31.83 | loss 258.83\n",
      "| epoch  32 |  300/ 427 batches | ms/batch 31.75 | loss 257.90\n",
      "| epoch  32 |  400/ 427 batches | ms/batch 32.00 | loss 256.28\n",
      "Training took 13.57 seconds.\n",
      "Training phase...\n",
      "| epoch  33 |  100/ 427 batches | ms/batch 32.21 | loss 261.78\n",
      "| epoch  33 |  200/ 427 batches | ms/batch 32.22 | loss 258.54\n",
      "| epoch  33 |  300/ 427 batches | ms/batch 32.11 | loss 255.62\n",
      "| epoch  33 |  400/ 427 batches | ms/batch 32.25 | loss 256.27\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  34 |  100/ 427 batches | ms/batch 32.63 | loss 257.46\n",
      "| epoch  34 |  200/ 427 batches | ms/batch 32.10 | loss 255.11\n",
      "| epoch  34 |  300/ 427 batches | ms/batch 32.42 | loss 261.18\n",
      "| epoch  34 |  400/ 427 batches | ms/batch 32.29 | loss 260.45\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  35 |  100/ 427 batches | ms/batch 32.75 | loss 255.62\n",
      "| epoch  35 |  200/ 427 batches | ms/batch 32.03 | loss 258.19\n",
      "| epoch  35 |  300/ 427 batches | ms/batch 32.58 | loss 256.90\n",
      "| epoch  35 |  400/ 427 batches | ms/batch 32.41 | loss 260.53\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  36 |  100/ 427 batches | ms/batch 32.82 | loss 261.02\n",
      "| epoch  36 |  200/ 427 batches | ms/batch 32.31 | loss 256.17\n",
      "| epoch  36 |  300/ 427 batches | ms/batch 32.45 | loss 258.51\n",
      "| epoch  36 |  400/ 427 batches | ms/batch 32.29 | loss 257.61\n",
      "Training took 13.81 seconds.\n",
      "Training phase...\n",
      "| epoch  37 |  100/ 427 batches | ms/batch 32.91 | loss 259.37\n",
      "| epoch  37 |  200/ 427 batches | ms/batch 32.41 | loss 257.55\n",
      "| epoch  37 |  300/ 427 batches | ms/batch 32.34 | loss 256.78\n",
      "| epoch  37 |  400/ 427 batches | ms/batch 32.59 | loss 258.18\n",
      "Training took 13.85 seconds.\n",
      "Training phase...\n",
      "| epoch  38 |  100/ 427 batches | ms/batch 32.84 | loss 258.49\n",
      "| epoch  38 |  200/ 427 batches | ms/batch 32.24 | loss 255.24\n",
      "| epoch  38 |  300/ 427 batches | ms/batch 32.38 | loss 258.32\n",
      "| epoch  38 |  400/ 427 batches | ms/batch 32.16 | loss 258.13\n",
      "Training took 13.78 seconds.\n",
      "Training phase...\n",
      "| epoch  39 |  100/ 427 batches | ms/batch 32.80 | loss 256.53\n",
      "| epoch  39 |  200/ 427 batches | ms/batch 32.28 | loss 257.76\n",
      "| epoch  39 |  300/ 427 batches | ms/batch 32.50 | loss 257.23\n",
      "| epoch  39 |  400/ 427 batches | ms/batch 32.37 | loss 258.97\n",
      "Training took 13.84 seconds.\n",
      "Training phase...\n",
      "| epoch  40 |  100/ 427 batches | ms/batch 32.70 | loss 257.43\n",
      "| epoch  40 |  200/ 427 batches | ms/batch 32.23 | loss 255.39\n",
      "| epoch  40 |  300/ 427 batches | ms/batch 32.22 | loss 258.53\n",
      "| epoch  40 |  400/ 427 batches | ms/batch 32.20 | loss 259.27\n",
      "Training took 13.76 seconds.\n",
      "Training phase...\n",
      "| epoch  41 |  100/ 427 batches | ms/batch 32.53 | loss 258.87\n",
      "| epoch  41 |  200/ 427 batches | ms/batch 32.26 | loss 258.26\n",
      "| epoch  41 |  300/ 427 batches | ms/batch 32.44 | loss 255.77\n",
      "| epoch  41 |  400/ 427 batches | ms/batch 32.45 | loss 257.96\n",
      "Training took 13.8 seconds.\n",
      "Training phase...\n",
      "| epoch  42 |  100/ 427 batches | ms/batch 32.70 | loss 257.99\n",
      "| epoch  42 |  200/ 427 batches | ms/batch 32.36 | loss 254.69\n",
      "| epoch  42 |  300/ 427 batches | ms/batch 32.33 | loss 257.56\n",
      "| epoch  42 |  400/ 427 batches | ms/batch 32.33 | loss 261.11\n",
      "Training took 13.81 seconds.\n",
      "Training phase...\n",
      "| epoch  43 |  100/ 427 batches | ms/batch 32.31 | loss 258.99\n",
      "| epoch  43 |  200/ 427 batches | ms/batch 31.88 | loss 259.85\n",
      "| epoch  43 |  300/ 427 batches | ms/batch 31.96 | loss 254.09\n",
      "| epoch  43 |  400/ 427 batches | ms/batch 31.92 | loss 256.89\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  44 |  100/ 427 batches | ms/batch 31.97 | loss 258.28\n",
      "| epoch  44 |  200/ 427 batches | ms/batch 32.22 | loss 256.97\n",
      "| epoch  44 |  300/ 427 batches | ms/batch 31.83 | loss 255.74\n",
      "| epoch  44 |  400/ 427 batches | ms/batch 31.95 | loss 258.72\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  45 |  100/ 427 batches | ms/batch 32.27 | loss 262.20\n",
      "| epoch  45 |  200/ 427 batches | ms/batch 31.88 | loss 255.12\n",
      "| epoch  45 |  300/ 427 batches | ms/batch 31.98 | loss 254.34\n",
      "| epoch  45 |  400/ 427 batches | ms/batch 32.17 | loss 258.16\n",
      "Training took 13.65 seconds.\n",
      "Training phase...\n",
      "| epoch  46 |  100/ 427 batches | ms/batch 32.79 | loss 259.79\n",
      "| epoch  46 |  200/ 427 batches | ms/batch 32.14 | loss 254.34\n",
      "| epoch  46 |  300/ 427 batches | ms/batch 31.88 | loss 259.68\n",
      "| epoch  46 |  400/ 427 batches | ms/batch 31.99 | loss 256.05\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  47 |  100/ 427 batches | ms/batch 32.22 | loss 257.06\n",
      "| epoch  47 |  200/ 427 batches | ms/batch 32.08 | loss 255.20\n",
      "| epoch  47 |  300/ 427 batches | ms/batch 31.79 | loss 256.52\n",
      "| epoch  47 |  400/ 427 batches | ms/batch 31.91 | loss 259.40\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  48 |  100/ 427 batches | ms/batch 32.15 | loss 259.23\n",
      "| epoch  48 |  200/ 427 batches | ms/batch 31.95 | loss 256.06\n",
      "| epoch  48 |  300/ 427 batches | ms/batch 32.07 | loss 257.15\n",
      "| epoch  48 |  400/ 427 batches | ms/batch 31.99 | loss 255.94\n",
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch  49 |  100/ 427 batches | ms/batch 32.21 | loss 259.36\n",
      "| epoch  49 |  200/ 427 batches | ms/batch 31.80 | loss 256.34\n",
      "| epoch  49 |  300/ 427 batches | ms/batch 31.78 | loss 257.42\n",
      "| epoch  49 |  400/ 427 batches | ms/batch 31.88 | loss 255.92\n",
      "Training took 13.58 seconds.\n",
      "Training phase...\n",
      "| epoch  50 |  100/ 427 batches | ms/batch 32.26 | loss 257.07\n",
      "| epoch  50 |  200/ 427 batches | ms/batch 32.05 | loss 257.90\n",
      "| epoch  50 |  300/ 427 batches | ms/batch 31.93 | loss 255.49\n",
      "| epoch  50 |  400/ 427 batches | ms/batch 32.00 | loss 259.32\n",
      "Training took 13.64 seconds.\n",
      "Training phase...\n",
      "| epoch  51 |  100/ 427 batches | ms/batch 32.43 | loss 259.81\n",
      "| epoch  51 |  200/ 427 batches | ms/batch 31.92 | loss 257.25\n",
      "| epoch  51 |  300/ 427 batches | ms/batch 31.82 | loss 256.57\n",
      "| epoch  51 |  400/ 427 batches | ms/batch 31.89 | loss 256.11\n",
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch  52 |  100/ 427 batches | ms/batch 32.22 | loss 261.27\n",
      "| epoch  52 |  200/ 427 batches | ms/batch 31.86 | loss 255.21\n",
      "| epoch  52 |  300/ 427 batches | ms/batch 31.90 | loss 256.53\n",
      "| epoch  52 |  400/ 427 batches | ms/batch 31.94 | loss 257.08\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  53 |  100/ 427 batches | ms/batch 32.46 | loss 259.09\n",
      "| epoch  53 |  200/ 427 batches | ms/batch 32.02 | loss 257.65\n",
      "| epoch  53 |  300/ 427 batches | ms/batch 31.95 | loss 255.00\n",
      "| epoch  53 |  400/ 427 batches | ms/batch 31.85 | loss 256.79\n",
      "Training took 13.64 seconds.\n",
      "Training phase...\n",
      "| epoch  54 |  100/ 427 batches | ms/batch 32.04 | loss 256.75\n",
      "| epoch  54 |  200/ 427 batches | ms/batch 31.82 | loss 257.71\n",
      "| epoch  54 |  300/ 427 batches | ms/batch 32.10 | loss 258.63\n",
      "| epoch  54 |  400/ 427 batches | ms/batch 31.92 | loss 256.48\n",
      "Training took 13.6 seconds.\n",
      "Training phase...\n",
      "| epoch  55 |  100/ 427 batches | ms/batch 32.51 | loss 260.79\n",
      "| epoch  55 |  200/ 427 batches | ms/batch 31.88 | loss 256.68\n",
      "| epoch  55 |  300/ 427 batches | ms/batch 32.11 | loss 255.50\n",
      "| epoch  55 |  400/ 427 batches | ms/batch 31.78 | loss 257.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch  56 |  100/ 427 batches | ms/batch 32.17 | loss 260.17\n",
      "| epoch  56 |  200/ 427 batches | ms/batch 31.94 | loss 258.48\n",
      "| epoch  56 |  300/ 427 batches | ms/batch 31.82 | loss 256.50\n",
      "| epoch  56 |  400/ 427 batches | ms/batch 31.87 | loss 254.89\n",
      "Training took 13.6 seconds.\n",
      "Training phase...\n",
      "| epoch  57 |  100/ 427 batches | ms/batch 32.20 | loss 255.64\n",
      "| epoch  57 |  200/ 427 batches | ms/batch 31.93 | loss 257.43\n",
      "| epoch  57 |  300/ 427 batches | ms/batch 32.14 | loss 256.83\n",
      "| epoch  57 |  400/ 427 batches | ms/batch 31.78 | loss 257.00\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  58 |  100/ 427 batches | ms/batch 32.26 | loss 257.55\n",
      "| epoch  58 |  200/ 427 batches | ms/batch 31.80 | loss 257.42\n",
      "| epoch  58 |  300/ 427 batches | ms/batch 31.82 | loss 257.83\n",
      "| epoch  58 |  400/ 427 batches | ms/batch 32.01 | loss 256.98\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  59 |  100/ 427 batches | ms/batch 32.23 | loss 258.89\n",
      "| epoch  59 |  200/ 427 batches | ms/batch 31.97 | loss 254.27\n",
      "| epoch  59 |  300/ 427 batches | ms/batch 31.93 | loss 257.32\n",
      "| epoch  59 |  400/ 427 batches | ms/batch 31.89 | loss 257.81\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  60 |  100/ 427 batches | ms/batch 32.03 | loss 258.04\n",
      "| epoch  60 |  200/ 427 batches | ms/batch 31.96 | loss 257.94\n",
      "| epoch  60 |  300/ 427 batches | ms/batch 31.76 | loss 257.03\n",
      "| epoch  60 |  400/ 427 batches | ms/batch 31.86 | loss 254.45\n",
      "Training took 13.58 seconds.\n",
      "Training phase...\n",
      "| epoch  61 |  100/ 427 batches | ms/batch 32.39 | loss 259.49\n",
      "| epoch  61 |  200/ 427 batches | ms/batch 31.67 | loss 255.23\n",
      "| epoch  61 |  300/ 427 batches | ms/batch 31.92 | loss 253.61\n",
      "| epoch  61 |  400/ 427 batches | ms/batch 31.88 | loss 259.45\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  62 |  100/ 427 batches | ms/batch 32.31 | loss 260.92\n",
      "| epoch  62 |  200/ 427 batches | ms/batch 31.96 | loss 255.26\n",
      "| epoch  62 |  300/ 427 batches | ms/batch 31.73 | loss 257.27\n",
      "| epoch  62 |  400/ 427 batches | ms/batch 31.88 | loss 255.31\n",
      "Training took 13.6 seconds.\n",
      "Training phase...\n",
      "| epoch  63 |  100/ 427 batches | ms/batch 32.29 | loss 260.76\n",
      "| epoch  63 |  200/ 427 batches | ms/batch 31.94 | loss 256.46\n",
      "| epoch  63 |  300/ 427 batches | ms/batch 32.12 | loss 255.87\n",
      "| epoch  63 |  400/ 427 batches | ms/batch 31.78 | loss 254.89\n",
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch  64 |  100/ 427 batches | ms/batch 32.11 | loss 257.95\n",
      "| epoch  64 |  200/ 427 batches | ms/batch 31.78 | loss 258.79\n",
      "| epoch  64 |  300/ 427 batches | ms/batch 31.85 | loss 255.66\n",
      "| epoch  64 |  400/ 427 batches | ms/batch 31.87 | loss 253.66\n",
      "Training took 13.57 seconds.\n",
      "Training phase...\n",
      "| epoch  65 |  100/ 427 batches | ms/batch 32.15 | loss 260.48\n",
      "| epoch  65 |  200/ 427 batches | ms/batch 31.90 | loss 252.36\n",
      "| epoch  65 |  300/ 427 batches | ms/batch 31.93 | loss 258.50\n",
      "| epoch  65 |  400/ 427 batches | ms/batch 32.02 | loss 256.44\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  66 |  100/ 427 batches | ms/batch 32.17 | loss 257.85\n",
      "| epoch  66 |  200/ 427 batches | ms/batch 32.02 | loss 256.24\n",
      "| epoch  66 |  300/ 427 batches | ms/batch 31.92 | loss 257.54\n",
      "| epoch  66 |  400/ 427 batches | ms/batch 32.04 | loss 255.42\n",
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch  67 |  100/ 427 batches | ms/batch 32.08 | loss 259.44\n",
      "| epoch  67 |  200/ 427 batches | ms/batch 31.83 | loss 258.28\n",
      "| epoch  67 |  300/ 427 batches | ms/batch 31.91 | loss 252.27\n",
      "| epoch  67 |  400/ 427 batches | ms/batch 32.21 | loss 257.18\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  68 |  100/ 427 batches | ms/batch 32.64 | loss 260.44\n",
      "| epoch  68 |  200/ 427 batches | ms/batch 31.98 | loss 252.55\n",
      "| epoch  68 |  300/ 427 batches | ms/batch 31.62 | loss 257.17\n",
      "| epoch  68 |  400/ 427 batches | ms/batch 31.87 | loss 258.52\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  69 |  100/ 427 batches | ms/batch 32.00 | loss 257.36\n",
      "| epoch  69 |  200/ 427 batches | ms/batch 31.85 | loss 259.81\n",
      "| epoch  69 |  300/ 427 batches | ms/batch 31.62 | loss 255.42\n",
      "| epoch  69 |  400/ 427 batches | ms/batch 31.79 | loss 254.82\n",
      "Training took 13.54 seconds.\n",
      "Training phase...\n",
      "| epoch  70 |  100/ 427 batches | ms/batch 31.93 | loss 258.34\n",
      "| epoch  70 |  200/ 427 batches | ms/batch 31.75 | loss 259.43\n",
      "| epoch  70 |  300/ 427 batches | ms/batch 31.99 | loss 255.00\n",
      "| epoch  70 |  400/ 427 batches | ms/batch 32.10 | loss 254.92\n",
      "Training took 13.59 seconds.\n",
      "Training phase...\n",
      "| epoch  71 |  100/ 427 batches | ms/batch 32.31 | loss 257.13\n",
      "| epoch  71 |  200/ 427 batches | ms/batch 31.93 | loss 258.58\n",
      "| epoch  71 |  300/ 427 batches | ms/batch 31.91 | loss 253.96\n",
      "| epoch  71 |  400/ 427 batches | ms/batch 31.89 | loss 257.11\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  72 |  100/ 427 batches | ms/batch 32.20 | loss 258.90\n",
      "| epoch  72 |  200/ 427 batches | ms/batch 32.05 | loss 256.93\n",
      "| epoch  72 |  300/ 427 batches | ms/batch 31.82 | loss 252.53\n",
      "| epoch  72 |  400/ 427 batches | ms/batch 31.93 | loss 256.76\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  73 |  100/ 427 batches | ms/batch 32.26 | loss 258.60\n",
      "| epoch  73 |  200/ 427 batches | ms/batch 31.75 | loss 252.75\n",
      "| epoch  73 |  300/ 427 batches | ms/batch 31.95 | loss 259.55\n",
      "| epoch  73 |  400/ 427 batches | ms/batch 31.89 | loss 255.54\n",
      "Training took 13.6 seconds.\n",
      "Training phase...\n",
      "| epoch  74 |  100/ 427 batches | ms/batch 32.23 | loss 257.75\n",
      "| epoch  74 |  200/ 427 batches | ms/batch 31.73 | loss 255.82\n",
      "| epoch  74 |  300/ 427 batches | ms/batch 31.81 | loss 258.68\n",
      "| epoch  74 |  400/ 427 batches | ms/batch 32.13 | loss 255.23\n",
      "Training took 13.6 seconds.\n",
      "Training phase...\n",
      "| epoch  75 |  100/ 427 batches | ms/batch 32.29 | loss 258.45\n",
      "| epoch  75 |  200/ 427 batches | ms/batch 31.91 | loss 254.54\n",
      "| epoch  75 |  300/ 427 batches | ms/batch 32.14 | loss 256.43\n",
      "| epoch  75 |  400/ 427 batches | ms/batch 32.09 | loss 256.32\n",
      "Training took 13.66 seconds.\n",
      "Training phase...\n",
      "| epoch  76 |  100/ 427 batches | ms/batch 32.20 | loss 258.67\n",
      "| epoch  76 |  200/ 427 batches | ms/batch 31.84 | loss 255.17\n",
      "| epoch  76 |  300/ 427 batches | ms/batch 31.74 | loss 257.33\n",
      "| epoch  76 |  400/ 427 batches | ms/batch 32.12 | loss 254.59\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  77 |  100/ 427 batches | ms/batch 32.13 | loss 255.45\n",
      "| epoch  77 |  200/ 427 batches | ms/batch 32.12 | loss 256.97\n",
      "| epoch  77 |  300/ 427 batches | ms/batch 31.86 | loss 256.02\n",
      "| epoch  77 |  400/ 427 batches | ms/batch 31.85 | loss 258.41\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  78 |  100/ 427 batches | ms/batch 32.17 | loss 257.24\n",
      "| epoch  78 |  200/ 427 batches | ms/batch 31.86 | loss 256.73\n",
      "| epoch  78 |  300/ 427 batches | ms/batch 32.00 | loss 256.82\n",
      "| epoch  78 |  400/ 427 batches | ms/batch 31.95 | loss 255.80\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  79 |  100/ 427 batches | ms/batch 32.27 | loss 253.80\n",
      "| epoch  79 |  200/ 427 batches | ms/batch 32.18 | loss 257.48\n",
      "| epoch  79 |  300/ 427 batches | ms/batch 32.47 | loss 254.50\n",
      "| epoch  79 |  400/ 427 batches | ms/batch 31.89 | loss 259.24\n",
      "Training took 13.71 seconds.\n",
      "Training phase...\n",
      "| epoch  80 |  100/ 427 batches | ms/batch 32.32 | loss 255.73\n",
      "| epoch  80 |  200/ 427 batches | ms/batch 32.05 | loss 256.55\n",
      "| epoch  80 |  300/ 427 batches | ms/batch 31.93 | loss 254.88\n",
      "| epoch  80 |  400/ 427 batches | ms/batch 32.05 | loss 259.07\n",
      "Training took 13.65 seconds.\n",
      "Training phase...\n",
      "| epoch  81 |  100/ 427 batches | ms/batch 32.46 | loss 259.76\n",
      "| epoch  81 |  200/ 427 batches | ms/batch 31.93 | loss 256.19\n",
      "| epoch  81 |  300/ 427 batches | ms/batch 31.92 | loss 254.22\n",
      "| epoch  81 |  400/ 427 batches | ms/batch 32.00 | loss 254.14\n",
      "Training took 13.65 seconds.\n",
      "Training phase...\n",
      "| epoch  82 |  100/ 427 batches | ms/batch 32.41 | loss 259.93\n",
      "| epoch  82 |  200/ 427 batches | ms/batch 31.89 | loss 254.55\n",
      "| epoch  82 |  300/ 427 batches | ms/batch 31.95 | loss 254.96\n",
      "| epoch  82 |  400/ 427 batches | ms/batch 32.01 | loss 255.88\n",
      "Training took 13.64 seconds.\n",
      "Training phase...\n",
      "| epoch  83 |  100/ 427 batches | ms/batch 32.26 | loss 262.03\n",
      "| epoch  83 |  200/ 427 batches | ms/batch 32.02 | loss 256.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  83 |  300/ 427 batches | ms/batch 32.05 | loss 251.57\n",
      "| epoch  83 |  400/ 427 batches | ms/batch 32.05 | loss 255.72\n",
      "Training took 13.66 seconds.\n",
      "Training phase...\n",
      "| epoch  84 |  100/ 427 batches | ms/batch 32.54 | loss 256.35\n",
      "| epoch  84 |  200/ 427 batches | ms/batch 31.88 | loss 257.73\n",
      "| epoch  84 |  300/ 427 batches | ms/batch 31.88 | loss 255.62\n",
      "| epoch  84 |  400/ 427 batches | ms/batch 32.00 | loss 254.66\n",
      "Training took 13.64 seconds.\n",
      "Training phase...\n",
      "| epoch  85 |  100/ 427 batches | ms/batch 32.33 | loss 258.68\n",
      "| epoch  85 |  200/ 427 batches | ms/batch 32.04 | loss 255.88\n",
      "| epoch  85 |  300/ 427 batches | ms/batch 32.22 | loss 257.05\n",
      "| epoch  85 |  400/ 427 batches | ms/batch 32.02 | loss 254.84\n",
      "Training took 13.68 seconds.\n",
      "Training phase...\n",
      "| epoch  86 |  100/ 427 batches | ms/batch 32.43 | loss 260.30\n",
      "| epoch  86 |  200/ 427 batches | ms/batch 31.93 | loss 254.44\n",
      "| epoch  86 |  300/ 427 batches | ms/batch 32.02 | loss 254.94\n",
      "| epoch  86 |  400/ 427 batches | ms/batch 32.06 | loss 256.46\n",
      "Training took 13.66 seconds.\n",
      "Training phase...\n",
      "| epoch  87 |  100/ 427 batches | ms/batch 32.08 | loss 255.55\n",
      "| epoch  87 |  200/ 427 batches | ms/batch 31.88 | loss 253.45\n",
      "| epoch  87 |  300/ 427 batches | ms/batch 31.78 | loss 260.09\n",
      "| epoch  87 |  400/ 427 batches | ms/batch 31.98 | loss 256.41\n",
      "Training took 13.59 seconds.\n",
      "Training phase...\n",
      "| epoch  88 |  100/ 427 batches | ms/batch 32.30 | loss 258.89\n",
      "| epoch  88 |  200/ 427 batches | ms/batch 32.04 | loss 254.67\n",
      "| epoch  88 |  300/ 427 batches | ms/batch 32.01 | loss 255.41\n",
      "| epoch  88 |  400/ 427 batches | ms/batch 31.82 | loss 256.61\n",
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch  89 |  100/ 427 batches | ms/batch 32.15 | loss 259.69\n",
      "| epoch  89 |  200/ 427 batches | ms/batch 31.82 | loss 252.75\n",
      "| epoch  89 |  300/ 427 batches | ms/batch 31.90 | loss 258.98\n",
      "| epoch  89 |  400/ 427 batches | ms/batch 32.22 | loss 253.65\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  90 |  100/ 427 batches | ms/batch 32.56 | loss 256.43\n",
      "| epoch  90 |  200/ 427 batches | ms/batch 31.85 | loss 253.04\n",
      "| epoch  90 |  300/ 427 batches | ms/batch 31.79 | loss 257.91\n",
      "| epoch  90 |  400/ 427 batches | ms/batch 31.87 | loss 258.18\n",
      "Training took 13.62 seconds.\n",
      "Training phase...\n",
      "| epoch  91 |  100/ 427 batches | ms/batch 31.90 | loss 259.81\n",
      "| epoch  91 |  200/ 427 batches | ms/batch 31.93 | loss 255.17\n",
      "| epoch  91 |  300/ 427 batches | ms/batch 31.87 | loss 257.68\n",
      "| epoch  91 |  400/ 427 batches | ms/batch 31.82 | loss 253.74\n",
      "Training took 13.57 seconds.\n",
      "Training phase...\n",
      "| epoch  92 |  100/ 427 batches | ms/batch 32.25 | loss 255.74\n",
      "| epoch  92 |  200/ 427 batches | ms/batch 31.78 | loss 255.14\n",
      "| epoch  92 |  300/ 427 batches | ms/batch 32.11 | loss 254.02\n",
      "| epoch  92 |  400/ 427 batches | ms/batch 32.04 | loss 260.53\n",
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch  93 |  100/ 427 batches | ms/batch 32.18 | loss 257.31\n",
      "| epoch  93 |  200/ 427 batches | ms/batch 31.88 | loss 255.01\n",
      "| epoch  93 |  300/ 427 batches | ms/batch 32.08 | loss 255.81\n",
      "| epoch  93 |  400/ 427 batches | ms/batch 31.83 | loss 256.26\n",
      "Training took 13.61 seconds.\n",
      "Training phase...\n",
      "| epoch  94 |  100/ 427 batches | ms/batch 32.27 | loss 258.03\n",
      "| epoch  94 |  200/ 427 batches | ms/batch 31.80 | loss 257.55\n",
      "| epoch  94 |  300/ 427 batches | ms/batch 31.88 | loss 255.54\n",
      "| epoch  94 |  400/ 427 batches | ms/batch 31.70 | loss 253.78\n",
      "Training took 13.57 seconds.\n",
      "Training phase...\n",
      "| epoch  95 |  100/ 427 batches | ms/batch 32.06 | loss 256.50\n",
      "| epoch  95 |  200/ 427 batches | ms/batch 31.73 | loss 256.51\n",
      "| epoch  95 |  300/ 427 batches | ms/batch 31.83 | loss 256.03\n",
      "| epoch  95 |  400/ 427 batches | ms/batch 31.87 | loss 255.57\n",
      "Training took 13.56 seconds.\n",
      "Training phase...\n",
      "| epoch  96 |  100/ 427 batches | ms/batch 32.28 | loss 257.76\n",
      "| epoch  96 |  200/ 427 batches | ms/batch 32.09 | loss 255.37\n",
      "| epoch  96 |  300/ 427 batches | ms/batch 31.87 | loss 254.98\n",
      "| epoch  96 |  400/ 427 batches | ms/batch 31.90 | loss 257.03\n",
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch  97 |  100/ 427 batches | ms/batch 32.27 | loss 257.12\n",
      "| epoch  97 |  200/ 427 batches | ms/batch 31.90 | loss 259.30\n",
      "| epoch  97 |  300/ 427 batches | ms/batch 32.04 | loss 255.49\n",
      "| epoch  97 |  400/ 427 batches | ms/batch 31.68 | loss 254.14\n",
      "Training took 13.6 seconds.\n",
      "Training phase...\n",
      "| epoch  98 |  100/ 427 batches | ms/batch 32.36 | loss 256.95\n",
      "| epoch  98 |  200/ 427 batches | ms/batch 31.88 | loss 255.32\n",
      "| epoch  98 |  300/ 427 batches | ms/batch 31.87 | loss 255.88\n",
      "| epoch  98 |  400/ 427 batches | ms/batch 32.06 | loss 254.93\n",
      "Training took 13.63 seconds.\n",
      "Training phase...\n",
      "| epoch  99 |  100/ 427 batches | ms/batch 32.37 | loss 257.84\n",
      "| epoch  99 |  200/ 427 batches | ms/batch 32.21 | loss 255.21\n",
      "| epoch  99 |  300/ 427 batches | ms/batch 31.79 | loss 254.73\n",
      "| epoch  99 |  400/ 427 batches | ms/batch 32.14 | loss 257.02\n",
      "Training took 13.67 seconds.\n",
      "Training phase...\n",
      "| epoch 100 |  100/ 427 batches | ms/batch 32.55 | loss 258.99\n",
      "| epoch 100 |  200/ 427 batches | ms/batch 32.50 | loss 255.58\n",
      "| epoch 100 |  300/ 427 batches | ms/batch 32.21 | loss 255.17\n",
      "| epoch 100 |  400/ 427 batches | ms/batch 32.28 | loss 255.25\n",
      "Training took 13.78 seconds.\n"
     ]
    }
   ],
   "source": [
    "model = TrainableMultVAE(base_params[\"encoder_dims\"], base_params[\"decoder_dims\"], base_params[\"dropout\"])\n",
    "optimizer = optim.Adam(model.parameters(), **train_params[\"optimizer_kwargs\"])\n",
    "\n",
    "model.fit(train_data, optimizer, criterion, val_data=None, n_epochs=train_params[\"n_epochs\"],\n",
    "          k=train_params[\"k\"], beta=train_params[\"beta\"])\n",
    "\n",
    "ndcg, err, hr = model.predict_metrics(train_data, validation_data)\n",
    "\n",
    "res = {}\n",
    "\n",
    "res[\"NDCG10\"] = ndcg\n",
    "res[\"ERR10\"] = err\n",
    "res[\"HR10\"] = hr\n",
    "res[\"dataset\"] = \"explicit_binarized_filtered\"\n",
    "\n",
    "with open(tmp_savepath+\"vae_explicit_binarized_filtered\", \"wb\") as handle:\n",
    "    pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811d0dc",
   "metadata": {},
   "source": [
    "### SLIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cc0acb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning all 2265 vectors took 1.42 minutes.\n",
      "In W matrix we have 53258 nonzero elements (1.038%).\n",
      "Computing top-k list for each user...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c449b753f94e0db410d24a18510894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...evaluation...\n",
      "0.18657369989503855 0.1522059816433877 0.2983388947966705\n"
     ]
    }
   ],
   "source": [
    "# set the parameters\n",
    "slim = ParallSynSLIM(l1_reg, l2_reg)\n",
    "\n",
    "# train the model\n",
    "slim.fit(train_data)\n",
    "\n",
    "# how many nonzero entires in W matirx\n",
    "proc = 100*slim.W.nnz/slim.W.shape[0]**2\n",
    "\n",
    "print(\"Computing top-k list for each user...\")\n",
    "# produce top k list for all users\n",
    "start = time.time()\n",
    "top_k_list = slim.calculate_top_k(train_data, ids_to_gs, ids_to_us, k=k)\n",
    "pred_time = time.time() - start\n",
    "\n",
    "print(\"...evaluation...\")\n",
    "ev = Evaluator(k=k, true=validation_list, predicted=top_k_list)\n",
    "ev.calculate_metrics()\n",
    "ngcg10, err10, hr10 = ev.ndcg, ev.err, ev.hr\n",
    "\n",
    "# save the obtained results\n",
    "res = {}\n",
    "res[\"ndcg10\"] = ngcg10\n",
    "res[\"err10\"] = err10\n",
    "res[\"hr10\"] = hr10\n",
    "res[\"W_zeros_percentage\"] = proc\n",
    "res[\"prediction_calc_time_seconds\"] = pred_time\n",
    "res[\"datset\"] = \"explicit_binarized_filtered\"\n",
    "\n",
    "\n",
    "print(ngcg10, err10, hr10)\n",
    "# save the obtained results\n",
    "with open(tmp_savepath_slim+\"SLIM_explicit_binarized_filtered\", \"wb\") as handle:\n",
    "    pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
