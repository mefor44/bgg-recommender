{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "import scipy \n",
    "import pickle\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "\n",
    "\n",
    "from metrics import Evaluator, NDCG, NDCG_user, NDCG_MAP, NDCG_AP_user\n",
    "from my_models import EASE, ParallSynSLIM\n",
    "from utils_VAE import BaseMultiVAE, TrainableMultVAE, loss_function, naive_sparse2tensor, sparse2torch_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "min_n_r_users = 200\n",
    "min_n_r_items = 200\n",
    "data_path = \"/home/mmarzec12/data/\"\n",
    "file_names = [\"ratings_chunk_1.csv\", \"ratings_chunk_2.csv\", \"ratings_chunk_3.csv\",\n",
    "              \"ratings_chunk_4.csv\", \"ratings_chunk_5.csv\"]\n",
    "\n",
    "savepath = \"/home/mmarzec12/models/comparison_literature/\"\n",
    "\n",
    "warnings.simplefilter('ignore',SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging files into 1 file and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file 1 and adding it to dataframe took 0.2 minutes.\n",
      "Loading file 2 and adding it to dataframe took 0.21 minutes.\n",
      "Loading file 3 and adding it to dataframe took 0.22 minutes.\n",
      "Loading file 4 and adding it to dataframe took 0.23 minutes.\n",
      "Loading file 5 and adding it to dataframe took 0.19 minutes.\n",
      "Final dataframe shape is (47623627, 12)\n",
      "We have 513378 different users (with ratings>0).\n",
      "We have 107330 different games.\n"
     ]
    }
   ],
   "source": [
    "cols = ['user_name', 'game_id', 'score', 'timestamp', 'stat_own',\n",
    "       'stat_preordered', 'stat_wishlist', 'stat_fortrade', 'stat_wanttoplay',\n",
    "       'stat_prevowned', 'stat_want', 'stat_wanttobuy']\n",
    "res = pd.DataFrame(columns=cols)\n",
    "start = perf_counter()\n",
    "for i, file_name in enumerate(file_names):\n",
    "    df = pd.read_csv(data_path+file_name)\n",
    "    filtered = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "    res = res.append(filtered)\n",
    "    end = perf_counter()\n",
    "    elapsed = end - start\n",
    "    print(\"Loading file {} and adding it to dataframe took {} minutes.\".format(i+1, round(elapsed/60, 2)))\n",
    "    start = perf_counter()\n",
    "print(\"Final dataframe shape is {}\".format(res.shape))\n",
    "print(\"We have {} different users (with ratings>0).\".format(len(res.user_name.unique())))\n",
    "print(\"We have {} different games.\".format(len(res.game_id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"game_id\"] = res[\"game_id\"].astype(int)\n",
    "res = res.reset_index()\n",
    "res = res.drop([\"index\"], axis=1)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing users and items from tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 iteration 414684 users and 80415 games were removed.\n",
      "After 2 iteration 1075 users and 3197 games were removed.\n",
      "After 3 iteration 38 users and 129 games were removed.\n",
      "We have started with 513378 users and 107330 games\n",
      "We have finished with 42790 users and 15613 games\n"
     ]
    }
   ],
   "source": [
    "i = 1 # iterations counter\n",
    "# initiate rating count for games\n",
    "tmp = res[(res.score>0) | (res.stat_own==1) | (res.stat_prevowned==1)].copy()\n",
    "items_g = tmp.groupby(\"game_id\")[\"score\"].count().reset_index().rename({\"score\":\"n_ratings\"}, axis=1)\n",
    "\n",
    "\n",
    "while items_g.n_ratings.min() < min_n_r_items:\n",
    "    # filtering games\n",
    "    filtered = items_g[items_g.n_ratings > min_n_r_items]\n",
    "    tmp = tmp.merge(filtered, on=\"game_id\", how=\"inner\").drop([\"n_ratings\"], axis=1)\n",
    "    diff_i = len(items_g) - len(filtered)\n",
    "    \n",
    "    # checking condition for users\n",
    "    users_g = tmp.groupby(\"user_name\")[\"score\"].count().reset_index().rename({\"score\":\"n_ratings\"}, axis=1)\n",
    "    if users_g.n_ratings.min() < min_n_r_users:\n",
    "        # filtering users\n",
    "        filtered = users_g[users_g.n_ratings > min_n_r_users]\n",
    "        tmp = tmp.merge(filtered, on=\"user_name\", how=\"inner\").drop([\"n_ratings\"], axis=1)\n",
    "        diff_u = len(users_g) - len(filtered)\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    print(\"After {} iteration {} users and {} games were removed.\".format(i, diff_u, diff_i))\n",
    "    \n",
    "    # recalculate ratings for games\n",
    "    items_g = tmp.groupby(\"game_id\")[\"score\"].count().reset_index().rename({\"score\":\"n_ratings\"}, axis=1)\n",
    "    \n",
    "    # increase iteration count\n",
    "    i += 1\n",
    "\n",
    "print(\"We have started with {} users and {} games\".format(len(res.user_name.unique()), len(res.game_id.unique())))\n",
    "print(\"We have finished with {} users and {} games\".format(len(tmp.user_name.unique()), len(tmp.game_id.unique())))\n",
    "\n",
    "del filtered, items_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = tmp.user_name.unique()\n",
    "unique_games = tmp.game_id.unique()\n",
    "\n",
    "n_users, n_items = len(unique_users), len(unique_games)\n",
    "\n",
    "# dictonaries to map users to unique ids and vice vers\n",
    "us_to_ids = {u:i for i,u in enumerate(unique_users)}\n",
    "ids_to_us = {i:u for i,u in enumerate(unique_users)}\n",
    "\n",
    "# dictonaries to map games to unique ids and vice vers\n",
    "gs_to_ids = {g:i for i,g in enumerate(unique_games)}\n",
    "ids_to_gs = {i:g for i,g in enumerate(unique_games)}\n",
    "\n",
    "# input length for sequencial models (BGG seq rec article)\n",
    "L = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp2.sort_values(\"timestamp\", ascending=True).groupby('user_name', sort=False).agg(lala=(\"user_name\", \"cumcount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tmp.sort_values(\"timestamp\", ascending=True).groupby('user_name', sort=False)\\\n",
    "      .agg(user_interaction_number=(\"user_name\", \"cumcount\")).join(tmp, how=\"inner\")\\\n",
    "      .sort_values(\"user_interaction_number\", ascending=True)\n",
    "res[\"user_interaction_number\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sample = res.shape[0] - n_users * L\n",
    "ids = np.array(range(n_users * L, res.shape[0]))\n",
    "np.random.shuffle(ids)\n",
    "train_ids = list(range(n_users*L)) + ids[:int(0.7*to_sample)].tolist()\n",
    "test_ids = ids[int(0.7*to_sample):int(0.9*to_sample)]\n",
    "val_ids = ids[int(0.9*to_sample):]\n",
    "\n",
    "# train test split (validation will not be used)\n",
    "train = res.iloc[train_ids,][[\"user_name\", \"game_id\"]]\n",
    "test = res.iloc[test_ids,][[\"user_name\", \"game_id\"]]\n",
    "val = res.iloc[val_ids,][[\"user_name\", \"game_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sparse matrix with data\n",
    "row = [us_to_ids[us] for us in train.user_name]\n",
    "col = [gs_to_ids[g] for g in train.game_id]\n",
    "data = [1] * train.shape[0]\n",
    "\n",
    "train_data = scipy.sparse.coo_matrix((data, (row, col)), shape=(len(unique_users), len(unique_games))).tocsr()\n",
    "\n",
    "\n",
    "tmp = test.groupby(\"user_name\").apply(lambda df: [gs_to_ids[g] for g in df.game_id.tolist()])    \n",
    "true = {us_to_ids[us]:games for us,games in tmp.iteritems()}\n",
    "\n",
    "tmp = val.groupby(\"user_name\").apply(lambda df: [gs_to_ids[g] for g in df.game_id.tolist()])\n",
    "val_dict = {us_to_ids[us]:games for us,games in tmp.iteritems()}\n",
    "\n",
    "del tmp, res, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scipy.sparse.save_npz(\"/home/mmarzec12/data/\"+\"train_data_big\", train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"/home/mmarzec12/data/\"+\"test_data_big\", \"wb\") as handle:\n",
    "#    pickle.dump(true, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open(\"/home/mmarzec12/data/\"+\"val_data_big\", \"wb\") as handle:\n",
    "#    pickle.dump(val_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = scipy.sparse.load_npz(\"/home/mmarzec12/data/\"+\"train_data_big.npz\")\n",
    "#savepath = \"/home/mmarzec12/data/\"\n",
    "#true = pd.read_pickle(\"/home/mmarzec12/data/\"+\"test_data_big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = pd.read_pickle(\"/home/mmarzec12/models/slim/\"+\"slim_best_params\")\n",
    "l1_reg = best_params[\"l1_reg\"]\n",
    "l2_reg = best_params[\"l2_reg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start = 18449.76455276\n",
      "end = 19151.315333362\n",
      "Learning all 15613 vectors took 11.71 minutes.\n",
      "In W matrix we have 2716014 nonzero elements (1.114%).\n",
      "Time elapsed = 11.72 minutes.\n"
     ]
    }
   ],
   "source": [
    "slim = ParallSynSLIM(l1_reg, l2_reg)\n",
    "        \n",
    "# train the model\n",
    "start = perf_counter()\n",
    "slim.fit(train_data)\n",
    "end = perf_counter()\n",
    "print(f\"Time elapsed = {round((end-start)/60, 2)} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a35b3dfd9e44556bf3bbe108f9a0049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 10\n",
    "recs = slim.calculate_top_k(train_data, ids_to_gs, ids_to_us, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = {us_to_ids[us]:[gs_to_ids[g] for g in games] for us,games in recs.items()}\n",
    "recs = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 42790/42790 [00:00<00:00, 123336.87it/s]\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "dict_metrics = NDCG_MAP(k, true, recs)\n",
    "\n",
    "res = {}\n",
    "res[\"model_name\"] = \"SLIM\"\n",
    "res[\"min_n_users\"] = min_n_r_users\n",
    "res[\"min_n_items\"] = min_n_r_items\n",
    "res[\"NDCG\"] = dict_metrics[\"NDCG\"]\n",
    "res[\"MAP\"] = dict_metrics[\"MAP\"]\n",
    "res[\"k\"] = k\n",
    "with open(savepath+\"SLIM_200\", \"wb\") as handle:\n",
    "    pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'SLIM',\n",
       " 'min_n_users': 200,\n",
       " 'min_n_items': 200,\n",
       " 'NDCG': 0.4462523373480932,\n",
       " 'MAP': 0.2824251614566555,\n",
       " 'k': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b210e10088424802b174f9a23e3928b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg = pd.read_pickle(\"/home/mmarzec12/models/ease/\"+\"ease_best_params\")\n",
    "k = 10\n",
    "gram = train_data.T @ train_data\n",
    "\n",
    "ease = EASE(regularization=reg)\n",
    "ease.fit(gram)\n",
    "del gram\n",
    "recs = ease.calculate_top_k(train_data, ids_to_gs, ids_to_us, k=k)\n",
    "\n",
    "tmp = {us_to_ids[us]:[gs_to_ids[g] for g in games] for us,games in recs.items()}\n",
    "recs = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 42790/42790 [00:00<00:00, 120120.95it/s]\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "dict_metrics = NDCG_MAP(k, true, recs)\n",
    "\n",
    "res = {}\n",
    "res[\"model_name\"] = \"EASE\"\n",
    "res[\"min_n_users\"] = min_n_r_users\n",
    "res[\"min_n_items\"] = min_n_r_items\n",
    "res[\"NDCG\"] = dict_metrics[\"NDCG\"]\n",
    "res[\"MAP\"] = dict_metrics[\"MAP\"]\n",
    "res[\"k\"] = k\n",
    "with open(savepath+\"EASE_200\", \"wb\") as handle:\n",
    "    pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'EASE',\n",
       " 'min_n_users': 200,\n",
       " 'min_n_items': 200,\n",
       " 'NDCG': 0.5461252600999648,\n",
       " 'MAP': 0.37754507061062187,\n",
       " 'k': 10}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase...\n",
      "| epoch   1 |  100/ 168 batches | ms/batch 179.21 | loss 2909.59\n",
      "Training took 29.6 seconds.\n",
      "Training phase...\n",
      "| epoch   2 |  100/ 168 batches | ms/batch 176.76 | loss 2752.71\n",
      "Training took 29.3 seconds.\n",
      "Training phase...\n",
      "| epoch   3 |  100/ 168 batches | ms/batch 176.71 | loss 2715.21\n",
      "Training took 29.31 seconds.\n",
      "Training phase...\n",
      "| epoch   4 |  100/ 168 batches | ms/batch 176.62 | loss 2697.63\n",
      "Training took 29.31 seconds.\n",
      "Training phase...\n",
      "| epoch   5 |  100/ 168 batches | ms/batch 176.52 | loss 2684.61\n",
      "Training took 29.3 seconds.\n",
      "Training phase...\n",
      "| epoch   6 |  100/ 168 batches | ms/batch 176.45 | loss 2666.85\n",
      "Training took 29.27 seconds.\n",
      "Training phase...\n",
      "| epoch   7 |  100/ 168 batches | ms/batch 176.38 | loss 2663.76\n",
      "Training took 29.28 seconds.\n",
      "Training phase...\n",
      "| epoch   8 |  100/ 168 batches | ms/batch 176.74 | loss 2666.00\n",
      "Training took 29.3 seconds.\n",
      "Training phase...\n",
      "| epoch   9 |  100/ 168 batches | ms/batch 176.28 | loss 2662.67\n",
      "Training took 29.31 seconds.\n",
      "Training phase...\n",
      "| epoch  10 |  100/ 168 batches | ms/batch 177.72 | loss 2654.48\n",
      "Training took 29.51 seconds.\n",
      "Training phase...\n",
      "| epoch  11 |  100/ 168 batches | ms/batch 176.86 | loss 2619.04\n",
      "Training took 29.33 seconds.\n",
      "Training phase...\n",
      "| epoch  12 |  100/ 168 batches | ms/batch 176.84 | loss 2623.42\n",
      "Training took 29.31 seconds.\n",
      "Training phase...\n",
      "| epoch  13 |  100/ 168 batches | ms/batch 176.56 | loss 2610.16\n",
      "Training took 29.36 seconds.\n",
      "Training phase...\n",
      "| epoch  14 |  100/ 168 batches | ms/batch 177.29 | loss 2615.62\n",
      "Training took 29.42 seconds.\n",
      "Training phase...\n",
      "| epoch  15 |  100/ 168 batches | ms/batch 177.12 | loss 2621.22\n",
      "Training took 29.37 seconds.\n",
      "Training phase...\n",
      "| epoch  16 |  100/ 168 batches | ms/batch 176.91 | loss 2595.73\n",
      "Training took 29.32 seconds.\n",
      "Training phase...\n",
      "| epoch  17 |  100/ 168 batches | ms/batch 179.33 | loss 2610.07\n",
      "Training took 29.69 seconds.\n",
      "Training phase...\n",
      "| epoch  18 |  100/ 168 batches | ms/batch 181.52 | loss 2592.08\n",
      "Training took 30.07 seconds.\n",
      "Training phase...\n",
      "| epoch  19 |  100/ 168 batches | ms/batch 180.09 | loss 2602.26\n",
      "Training took 29.95 seconds.\n",
      "Training phase...\n",
      "| epoch  20 |  100/ 168 batches | ms/batch 181.75 | loss 2594.50\n",
      "Training took 30.13 seconds.\n",
      "Training phase...\n",
      "| epoch  21 |  100/ 168 batches | ms/batch 180.40 | loss 2611.07\n",
      "Training took 29.9 seconds.\n",
      "Training phase...\n",
      "| epoch  22 |  100/ 168 batches | ms/batch 180.87 | loss 2601.07\n",
      "Training took 29.89 seconds.\n",
      "Training phase...\n",
      "| epoch  23 |  100/ 168 batches | ms/batch 179.10 | loss 2595.58\n",
      "Training took 29.82 seconds.\n",
      "Training phase...\n",
      "| epoch  24 |  100/ 168 batches | ms/batch 179.53 | loss 2588.98\n",
      "Training took 29.68 seconds.\n",
      "Training phase...\n",
      "| epoch  25 |  100/ 168 batches | ms/batch 180.03 | loss 2575.26\n",
      "Training took 29.82 seconds.\n",
      "Training phase...\n",
      "| epoch  26 |  100/ 168 batches | ms/batch 178.96 | loss 2558.95\n",
      "Training took 29.72 seconds.\n",
      "Training phase...\n",
      "| epoch  27 |  100/ 168 batches | ms/batch 179.91 | loss 2575.60\n",
      "Training took 29.87 seconds.\n",
      "Training phase...\n",
      "| epoch  28 |  100/ 168 batches | ms/batch 178.78 | loss 2586.77\n",
      "Training took 29.7 seconds.\n",
      "Training phase...\n",
      "| epoch  29 |  100/ 168 batches | ms/batch 179.41 | loss 2559.21\n",
      "Training took 30.16 seconds.\n",
      "Training phase...\n",
      "| epoch  30 |  100/ 168 batches | ms/batch 186.04 | loss 2569.18\n",
      "Training took 30.56 seconds.\n",
      "Training phase...\n",
      "| epoch  31 |  100/ 168 batches | ms/batch 181.50 | loss 2568.60\n",
      "Training took 30.04 seconds.\n",
      "Training phase...\n",
      "| epoch  32 |  100/ 168 batches | ms/batch 179.48 | loss 2572.90\n",
      "Training took 29.84 seconds.\n",
      "Training phase...\n",
      "| epoch  33 |  100/ 168 batches | ms/batch 178.43 | loss 2565.15\n",
      "Training took 29.67 seconds.\n",
      "Training phase...\n",
      "| epoch  34 |  100/ 168 batches | ms/batch 178.84 | loss 2554.89\n",
      "Training took 29.7 seconds.\n",
      "Training phase...\n",
      "| epoch  35 |  100/ 168 batches | ms/batch 178.31 | loss 2552.98\n",
      "Training took 29.67 seconds.\n",
      "Training phase...\n",
      "| epoch  36 |  100/ 168 batches | ms/batch 180.33 | loss 2552.21\n",
      "Training took 29.93 seconds.\n",
      "Training phase...\n",
      "| epoch  37 |  100/ 168 batches | ms/batch 179.51 | loss 2550.36\n",
      "Training took 29.8 seconds.\n",
      "Training phase...\n",
      "| epoch  38 |  100/ 168 batches | ms/batch 179.58 | loss 2543.29\n",
      "Training took 29.9 seconds.\n",
      "Training phase...\n",
      "| epoch  39 |  100/ 168 batches | ms/batch 180.37 | loss 2528.28\n",
      "Training took 29.93 seconds.\n",
      "Training phase...\n",
      "| epoch  40 |  100/ 168 batches | ms/batch 180.68 | loss 2545.54\n",
      "Training took 30.04 seconds.\n",
      "Training phase...\n",
      "| epoch  41 |  100/ 168 batches | ms/batch 179.83 | loss 2538.27\n",
      "Training took 29.91 seconds.\n",
      "Training phase...\n",
      "| epoch  42 |  100/ 168 batches | ms/batch 180.30 | loss 2544.49\n",
      "Training took 29.74 seconds.\n",
      "Training phase...\n",
      "| epoch  43 |  100/ 168 batches | ms/batch 176.47 | loss 2543.68\n",
      "Training took 29.27 seconds.\n",
      "Training phase...\n",
      "| epoch  44 |  100/ 168 batches | ms/batch 176.49 | loss 2554.71\n",
      "Training took 29.3 seconds.\n",
      "Training phase...\n",
      "| epoch  45 |  100/ 168 batches | ms/batch 176.15 | loss 2545.05\n",
      "Training took 29.21 seconds.\n",
      "Training phase...\n",
      "| epoch  46 |  100/ 168 batches | ms/batch 176.41 | loss 2543.50\n",
      "Training took 29.27 seconds.\n",
      "Training phase...\n",
      "| epoch  47 |  100/ 168 batches | ms/batch 176.25 | loss 2537.77\n",
      "Training took 29.24 seconds.\n",
      "Training phase...\n",
      "| epoch  48 |  100/ 168 batches | ms/batch 176.59 | loss 2538.92\n",
      "Training took 29.31 seconds.\n",
      "Training phase...\n",
      "| epoch  49 |  100/ 168 batches | ms/batch 176.32 | loss 2530.56\n",
      "Training took 29.24 seconds.\n",
      "Training phase...\n",
      "| epoch  50 |  100/ 168 batches | ms/batch 176.27 | loss 2541.94\n",
      "Training took 29.25 seconds.\n"
     ]
    }
   ],
   "source": [
    "best_params = pd.read_pickle(\"/home/mmarzec12/models/vae/model_tuning/\"+\"vae_best_params\")\n",
    "base_params = best_params[\"base_params\"]\n",
    "base_params[\"encoder_dims\"][0] = n_items\n",
    "base_params[\"decoder_dims\"][-1] = n_items\n",
    "train_params = best_params[\"train_params\"]\n",
    "train_params[\"n_epochs\"] = 50\n",
    "\n",
    "model = TrainableMultVAE(base_params[\"encoder_dims\"], base_params[\"decoder_dims\"], base_params[\"dropout\"])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), **train_params[\"optimizer_kwargs\"])\n",
    "criterion = loss_function\n",
    "model.fit(train_data, optimizer, criterion, val_data=None, n_epochs=train_params[\"n_epochs\"],\n",
    "          k=train_params[\"k\"], beta=train_params[\"beta\"])\n",
    "\n",
    "recs = model.predict_dict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 42790/42790 [00:00<00:00, 96586.47it/s]\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "metrics = NDCG_MAP(k, true, recs)\n",
    "\n",
    "res = {}\n",
    "res[\"model_name\"] = \"VAE\"\n",
    "res[\"min_n_users\"] = min_n_r_users\n",
    "res[\"min_n_items\"] = min_n_r_items\n",
    "res[\"NDCG\"] = metrics[\"NDCG\"]\n",
    "res[\"MAP\"] = metrics[\"MAP\"]\n",
    "res[\"k\"] = k\n",
    "with open(savepath+\"VAE_200\", \"wb\") as handle:\n",
    "    pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.4172961831571981, 'MAP': 0.2533637689788075}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = pd.read_csv(\"/home/mmarzec12/data/games_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_AP_user(k, true, predicted, denoms):\n",
    "    idcg = sum(denoms[:min(len(true),k)])\n",
    "    true_set = set(true)\n",
    "    dcg = 0\n",
    "    n_relevant = 0\n",
    "    ap = 0\n",
    "    for i,item in enumerate(predicted):\n",
    "        if item in true_set:\n",
    "            dcg += denoms[i]\n",
    "            n_relevant += 1\n",
    "            ap += (n_relevant/(i+1))\n",
    "    \n",
    "    if n_relevant == 0:\n",
    "        n_relevant = 1\n",
    "    return {\"NDCGu\":dcg / idcg, \"APu\":ap / n_relevant}\n",
    "    \n",
    "def NDCG_MAP(k, true_dict, predicted_dict):\n",
    "    \n",
    "    denoms = 1. / np.log2(np.arange(2, k + 2))\n",
    "    n_users = len(true_dict.keys())\n",
    "    ndcg_ = 0\n",
    "    map_ = 0\n",
    "    \n",
    "    for uid in tqdm(true_dict.keys()):\n",
    "        res = NDCG_AP_user(k, true_dict[uid], predicted_dict[uid], denoms)\n",
    "        ndcg_ += res[\"NDCGu\"]\n",
    "        map_ + res[\"APu\"]\n",
    "        \n",
    "    return {\"NDCG\": ndcg_/n_users, \"MAP\": map_/n_users}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
