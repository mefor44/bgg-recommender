{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfb36b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from metrics import Evaluator\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16391850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 1362961 rows with score <= 6.\n"
     ]
    }
   ],
   "source": [
    "mypath = \"/home/mmarzec12/data/\"\n",
    "savepath = \"/home/mmarzec12/models/jncf/\"\n",
    "\n",
    "explicit = pd.read_csv(mypath+\"explicit_train.csv\")\n",
    "validation = pd.read_csv(mypath+\"leave_one_out_validation.csv\")\n",
    "\n",
    "\n",
    "# list with (user,item) tuples from validation set\n",
    "validation_list = [(u,i) for u,i in zip(validation.user_name, validation.game_id)]\n",
    "# dict with user:game key-value pairs from validation set\n",
    "validation_dict = {u:i for u,i in zip(validation.user_name, validation.game_id)}\n",
    "\n",
    "# unique games and users\n",
    "unique_users = explicit.user_name.unique()\n",
    "unique_games = explicit.game_id.unique()\n",
    "\n",
    "n_users, n_items = len(unique_users), len(unique_games)\n",
    "\n",
    "# dictonaries to map users to unique ids and vice vers\n",
    "us_to_ids = {u:i for i,u in enumerate(unique_users)}\n",
    "ids_to_us = {i:u for i,u in enumerate(unique_users)}\n",
    "\n",
    "# dictonaries to map games to unique ids and vice vers\n",
    "gs_to_ids = {g:i for i,g in enumerate(unique_games)}\n",
    "ids_to_gs = {i:g for i,g in enumerate(unique_games)}\n",
    "\n",
    "\n",
    "implicit = pd.read_csv(mypath+\"implicit_train.csv\")\n",
    "\n",
    "# filtering explicit ratings: filter ratings <6 and >=1\n",
    "print(f\"There is {np.sum(explicit.score <= 6)} rows with score <= 6.\")\n",
    "explicit = explicit[explicit.score > 6]\n",
    "\n",
    "# we join implictit and explicit rating data\n",
    "joined = pd.concat([explicit, implicit])\n",
    "joined = joined[[\"user_name\", \"game_id\", \"score\"]]\n",
    "# converting all interaction data to \"1\" \n",
    "joined[\"score\"] = 1\n",
    "\n",
    "# creating sparse matrix with data\n",
    "row = [us_to_ids[us] for us in joined.user_name]\n",
    "col = [gs_to_ids[g] for g in joined.game_id]\n",
    "data = joined.score\n",
    "\n",
    "user_matrix = scipy.sparse.coo_matrix((data, (row, col)), shape=(len(unique_users), len(unique_games))).tocsr()\n",
    "item_matrix = user_matrix.T.copy()\n",
    "dok_matrix = user_matrix.todok()\n",
    "\n",
    "user_loc = row\n",
    "item_loc = col\n",
    "ratings = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fe4d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e223ab87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "624be41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JNCF(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items, DF_layers=[128, 64, 32], DI_layers=[64, 8], combination=\"concatenation\"):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.combination = combination\n",
    "        self.embed_dim = DF_layers[-1]\n",
    "        \n",
    "        if self.combination == 'concatenation':\n",
    "            self.embed_dim *= 2\n",
    "        elif self.combination == 'multiplication':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('combination type should be \"concatenation\" or \"multiplication\" !')\n",
    "        \n",
    "        self.DI_layers = self.initialize_layers(self.embed_dim, DI_layers)\n",
    "        self.DF_users = self.initialize_layers(n_items, DF_layers)\n",
    "        self.DF_items = self.initialize_layers(n_users, DF_layers)\n",
    "        self.prediction_layer = nn.Linear(DI_layers[-1], 1)\n",
    "        \n",
    "        \n",
    "    def initialize_layers(self, n_initial, layers, nonlinearity=\"relu\"):\n",
    "        res = []\n",
    "        for i in range(len(layers)):\n",
    "            if i == 0:\n",
    "                layer = nn.Linear(n_initial, layers[i])\n",
    "            else:\n",
    "                layer = nn.Linear(layers[i-1], layers[i])\n",
    "                \n",
    "            nn.init.normal_(layer.weight, 0, 0.01)\n",
    "            layer.bias.data.normal_(0.0, 0.01)\n",
    "            res.append(layer)\n",
    "            res.append(nn.ReLU())\n",
    "        \n",
    "        #print(res)\n",
    "        return nn.Sequential(*res)\n",
    "    \n",
    "    \n",
    "    def forward(self, user, item_i, item_j):\n",
    "        zu = self.DF_users(user)\n",
    "        zi = self.DF_items(item_i)\n",
    "        zj = self.DF_items(item_j)\n",
    "        \n",
    "        if self.combination == \"concatenation\":\n",
    "            i_feature_vector = torch.cat((zu, zi), dim=-1)\n",
    "            j_feature_vector = torch.cat((zu, zj), dim=-1)\n",
    "        elif self.combination == \"multiplication\":\n",
    "            i_feature_vector = zu * zi\n",
    "            j_feature_vector = zu * zj\n",
    "        \n",
    "        y_i = self.prediction_layer(self.DI_layers(i_feature_vector))\n",
    "        y_j = self.prediction_layer(self.DI_layers(j_feature_vector))\n",
    "        return y_i.view(-1), y_j.view(-1)\n",
    "    \n",
    "# utlis function\n",
    "def TOP1(item_i, item_j, n_negs):\n",
    "    diff = item_j - item_i\n",
    "    loss = (torch.sigmoid(diff) + torch.sigmoid(torch.pow(item_j, 2)))\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f621a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idxlist, item_idxlist = list(range(n_users)), list(range(n_items))\n",
    "\n",
    "# list with indices, len=numb_of_obs\n",
    "idxlist = np.array(range(len(user_loc)))\n",
    "\n",
    "# model\n",
    "model = JNCF(n_users, n_items)\n",
    "\n",
    "lr = 1e-3\n",
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "n_negs = 10\n",
    "alpha = 0.8\n",
    "\n",
    "# validation\n",
    "sample_size_val = 100\n",
    "k = 10\n",
    "NDCGs = []\n",
    "ERRs = []\n",
    "HRs = []\n",
    "\n",
    "point_loss_function = nn.NLLLoss() # nn.MSELoss()\n",
    "pair_loss_function = TOP1\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01ac89d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5900f0c4a94d579625b5f093146737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73544 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_861/481164904.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0mnegid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdok_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                         \u001b[0mneg_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    np.random.shuffle(idxlist)\n",
    "    epoch_loss, epoch_pair_loss, epoch_point_loss = .0, .0, .0\n",
    "    \n",
    "    # Training phase\n",
    "    \n",
    "    print(\"Training phase...\")\n",
    "    start = perf_counter()\n",
    "    for batch_idx, start_idx in enumerate(tqdm(range(0, len(idxlist), batch_size))):\n",
    "        end_idx = min(len(idxlist), start_idx+batch_size)\n",
    "        idx = idxlist[start_idx:end_idx]\n",
    "        \n",
    "        # u_ids - list of user ids, but taken from the list of user ids from\n",
    "        # all interactions, so there are multiple instances of specific user id.\n",
    "        # This means that users with higher number of interactions are more likely\n",
    "        # to be picked here.\n",
    "        u_ids = user_loc[start_idx:end_idx]\n",
    "        i_ids = item_loc[start_idx:end_idx]\n",
    "        rs = ratings[start_idx:end_idx]\n",
    "        \n",
    "        # We select input for the network. In case of users we pick all\n",
    "        # their interaction history, in case of items all their interaction \n",
    "        # history (item input has length=n_users)\n",
    "        users = torch.FloatTensor(user_matrix[u_ids,].toarray())\n",
    "        #print(users.shape)\n",
    "        items = torch.FloatTensor(item_matrix[i_ids,].toarray())\n",
    "        labels = torch.LongTensor(rs)\n",
    "        \n",
    "    \n",
    "         # Negative Sampling\n",
    "        neg_items_list = []\n",
    "        to_sample = list(range(n_items))\n",
    "        for _ in range(0, n_negs):\n",
    "            neg_res = []\n",
    "            for u in u_ids:\n",
    "                cond = 0 \n",
    "                while cond == 0:\n",
    "                    negid = np.random.choice(to_sample)\n",
    "                    if dok_matrix[u, negid] != 0:\n",
    "                        neg_res.append(negid)\n",
    "                        cond = 1\n",
    "            neg_items_list.append(neg_res)\n",
    "\n",
    "        for neg_idx in range(0, n_negs):\n",
    "            # we start learning procedure\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            point_loss, pair_loss = 0., 0.\n",
    "\n",
    "            neg_ids = neg_items_list[neg_idx]\n",
    "            items_j = torch.FloatTensor(item_matrix[neg_ids].toarray())\n",
    "\n",
    "            y_i, y_j = model.forward(users, items, items_j)\n",
    "\n",
    "            point_loss = point_loss_function(y_i, labels)  # positive items i\n",
    "            pair_loss = pair_loss_function(y_i, y_j, n_negs)\n",
    "\n",
    "            loss = alpha * pair_loss + (1 - alpha) * point_loss\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_pair_loss += pair_loss.item()\n",
    "            epoch_point_loss += point_loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "    train_time = perf_counter() - start_time\n",
    "        \n",
    "    \n",
    "    # Evaluation phase\n",
    "    print(\"Evaluation phase...\")\n",
    "    model.eval()\n",
    "    val_res = {}\n",
    "    to_sample = list(ids_to_gs.keys())\n",
    "    for user in tqdm(unique_users):\n",
    "        #print(user)\n",
    "        selected = np.random.choice(to_sample, sample_size_val, replace=False)\n",
    "        selected  = np.append(selected, gs_to_ids[validation_dict[user]])\n",
    "        \n",
    "        \n",
    "        usr_id = [us_to_ids[user]] * len(selected)\n",
    "        usr = torch.FloatTensor(user_matrix[usr_id].toarray())\n",
    "        \n",
    "        itms = torch.FloatTensor(item_matrix[selected].toarray())\n",
    "        #print(usr.shape, itms.shape)\n",
    "        \n",
    "        preds = model.forward(usr, itms)\n",
    "        #print(preds.shape)\n",
    "        _, rec_ids = torch.topk(preds, k)\n",
    "        rec_ids = [ids_to_gs[selected[i]] for i in rec_ids]\n",
    "        val_res[user] = rec_ids\n",
    "    \n",
    "    evaluator = Evaluator(k=k, true=validation_list, predicted=val_res)\n",
    "    evaluator.calculate_metrics()\n",
    "    ndcg10, err10, hr10 = ev.ndcg, ev.err, ev.hr\n",
    "    \n",
    "    NDCGs.append(ndcg10)\n",
    "    ERRs.append(err10)\n",
    "    HRs.append(hr10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7b27069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = scipy.sparse.coo_matrix((data, (row, col)), shape=(len(unique_users), len(unique_games))).tocsr().toarray()\n",
    "item_matrix = user_matrix.T.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373c404c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e96dc54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1971015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation phase...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514502fd2d9a45a991e6aaf85fad2122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_871/2433490405.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0musr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0musr_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mitms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;31m#print(usr.shape, itms.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    np.random.shuffle(idxlist)\n",
    "    \n",
    "    #start = perf_counter()\n",
    "    # Training phase\n",
    "    \"\"\"\n",
    "    print(\"Training phase...\")\n",
    "    for batch_idx, start_idx in enumerate(tqdm(range(0, len(idxlist), batch_size))):\n",
    "        end_idx = min(len(idxlist), start_idx+batch_size)\n",
    "        idx = idxlist[start_idx:end_idx]\n",
    "        \n",
    "        # u_ids - list of user ids, but taken from the list of user ids from\n",
    "        # all interactions, so there are multiple instances of specific user id.\n",
    "        # This means that users with higher number of interactions are more likely\n",
    "        # to be picked here.\n",
    "        u_ids = user_loc[start_idx:end_idx]\n",
    "        i_ids = item_loc[start_idx:end_idx]\n",
    "        rs = ratings[start_idx:end_idx]\n",
    "        \n",
    "        # We select input for the network. In case of users we pick all\n",
    "        # their interaction history, in case of items all their interaction \n",
    "        # history (item input has length=n_users)\n",
    "        users = torch.FloatTensor(user_matrix[u_ids,])\n",
    "        #print(users.shape)\n",
    "        items = torch.FloatTensor(item_matrix[i_ids,])\n",
    "        labels = torch.LongTensor(rs)\n",
    "        \n",
    "        # todo: implement pair-wise learning\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        y_hat = model.forward(users, items)\n",
    "        #print(y_hat)\n",
    "        loss = pointwise_loss(y_hat, labels) \n",
    "        #print(batch_idx)\n",
    "        #print(f\"batch loss = {loss.item()}\")\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    # Evaluation phase\n",
    "    print(\"Evaluation phase...\")\n",
    "    model.eval()\n",
    "    val_res = {}\n",
    "    to_sample = list(ids_to_gs.keys())\n",
    "    for user in tqdm(unique_users):\n",
    "        #print(user)\n",
    "        selected = np.random.choice(to_sample, sample_size_val, replace=False)\n",
    "        selected  = np.append(selected, gs_to_ids[validation_dict[user]])\n",
    "        \n",
    "        \n",
    "        usr_id = [us_to_ids[user]] * len(selected)\n",
    "        usr = torch.FloatTensor(user_matrix[usr_id])\n",
    "        \n",
    "        itms = torch.FloatTensor(item_matrix[selected])\n",
    "        #print(usr.shape, itms.shape)\n",
    "        \n",
    "        preds = model.forward(usr, itms)\n",
    "        #print(preds.shape)\n",
    "        _, rec_ids = torch.topk(preds, k)\n",
    "        rec_ids = [ids_to_gs[selected[i]] for i in rec_ids]\n",
    "        val_res[user] = rec_ids\n",
    "    \n",
    "    evaluator = Evaluator(k=k, true=validation_list, predicted=val_res)\n",
    "    evaluator.calculate_metrics()\n",
    "    ndcg10, err10, hr10 = ev.ndcg, ev.err, ev.hr\n",
    "    \n",
    "    NDCGs.append(ndcg10)\n",
    "    ERRs.append(err10)\n",
    "    HRs.append(hr10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3db9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cba2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
