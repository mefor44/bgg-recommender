{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9b46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from metrics import Evaluator\n",
    "from utils_VAE import BaseMultiVAE, TrainableMultVAE, loss_function, naive_sparse2tensor, sparse2torch_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913d8c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 1362961 rows with score <= 6.\n"
     ]
    }
   ],
   "source": [
    "mypath = \"/home/mmarzec12/data/\"\n",
    "savepath = \"/home/mmarzec12/models/vae/\"\n",
    "\n",
    "explicit = pd.read_csv(mypath+\"explicit_train.csv\")\n",
    "validation = pd.read_csv(mypath+\"leave_one_out_validation.csv\")\n",
    "\n",
    "\n",
    "# list with (user,item) tuples from validation set\n",
    "validation_list = [(u,i) for u,i in zip(validation.user_name, validation.game_id)]\n",
    "# dict with user:game key-value pairs from validation set\n",
    "validation_dict = {u:i for u,i in zip(validation.user_name, validation.game_id)}\n",
    "\n",
    "# unique games and users\n",
    "unique_users = explicit.user_name.unique()\n",
    "unique_games = explicit.game_id.unique()\n",
    "\n",
    "n_users, n_items = len(unique_users), len(unique_games)\n",
    "\n",
    "# dictonaries to map users to unique ids and vice vers\n",
    "us_to_ids = {u:i for i,u in enumerate(unique_users)}\n",
    "ids_to_us = {i:u for i,u in enumerate(unique_users)}\n",
    "\n",
    "# dictonaries to map games to unique ids and vice vers\n",
    "gs_to_ids = {g:i for i,g in enumerate(unique_games)}\n",
    "ids_to_gs = {i:g for i,g in enumerate(unique_games)}\n",
    "\n",
    "\n",
    "implicit = pd.read_csv(mypath+\"implicit_train.csv\")\n",
    "implicit[\"score\"] = 1\n",
    "\n",
    "# filtering explicit ratings: filter ratings <6 and >=1\n",
    "print(f\"There is {np.sum(explicit.score <= 6)} rows with score <= 6.\")\n",
    "explicit = explicit[explicit.score > 6]\n",
    "\n",
    "# we join implictit and explicit rating data\n",
    "joined = pd.concat([explicit, implicit])\n",
    "joined = joined[[\"user_name\", \"game_id\", \"score\"]]\n",
    "# converting all interaction data to \"1\" \n",
    "joined[\"score\"] = 1\n",
    "\n",
    "# creating sparse matrix with data\n",
    "row = [us_to_ids[us] for us in joined.user_name]\n",
    "col = [gs_to_ids[g] for g in joined.game_id]\n",
    "data = joined.score\n",
    "\n",
    "train_data = scipy.sparse.coo_matrix((data, (row, col)), shape=(len(unique_users), len(unique_games))).tocsr()\n",
    "#item_matrix = user_matrix.T.copy()\n",
    "#dok_matrix = user_matrix.todok()\n",
    "\n",
    "user_loc = row\n",
    "item_loc = col\n",
    "ratings = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cd79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase...\n",
      "| epoch   1 |  100/ 427 batches | ms/batch 19.12 | loss 642.20\n",
      "| epoch   1 |  200/ 427 batches | ms/batch 18.03 | loss 621.39\n",
      "| epoch   1 |  300/ 427 batches | ms/batch 18.21 | loss 611.36\n",
      "| epoch   1 |  400/ 427 batches | ms/batch 18.35 | loss 608.24\n",
      "Training took 7.83 seconds.\n",
      "Evaluation phase...\n",
      "Validating took 2.82 seconds.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 10.65s | NDCG@10 0.128 | ERR@10 0.106 | HR@10 0.195\n",
      "-----------------------------------------------------------------------------------------\n",
      "Training phase...\n",
      "| epoch   2 |  100/ 427 batches | ms/batch 18.49 | loss 609.96\n",
      "| epoch   2 |  200/ 427 batches | ms/batch 18.34 | loss 605.07\n",
      "| epoch   2 |  300/ 427 batches | ms/batch 18.15 | loss 596.36\n",
      "| epoch   2 |  400/ 427 batches | ms/batch 18.28 | loss 602.22\n",
      "Training took 7.79 seconds.\n",
      "Evaluation phase...\n",
      "Validating took 2.82 seconds.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 10.61s | NDCG@10 0.140 | ERR@10 0.117 | HR@10 0.212\n",
      "-----------------------------------------------------------------------------------------\n",
      "Training phase...\n",
      "| epoch   3 |  100/ 427 batches | ms/batch 18.34 | loss 600.33\n",
      "| epoch   3 |  200/ 427 batches | ms/batch 18.37 | loss 596.96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8671/3344890463.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/scripts/utils_VAE.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, optimizer, criterion, n_epochs, batch_size, total_anneal_steps, anneal_cap, log_interval, k, val_data, beta)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training phase...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training took {round(time.time() - epoch_start_time,2)} seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_time_seconds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/utils_VAE.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, train_data, idxlist, epoch_num, update_count)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# it to tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaive_sparse2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# annealing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/utils_VAE.py\u001b[0m in \u001b[0;36mnaive_sparse2tensor\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnaive_sparse2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latent_dim = 200\n",
    "dim_layers = [300, 600]\n",
    "encoder_dims = [n_items] + dim_layers + [latent_dim]\n",
    "decoder_dims = [latent_dim] + dim_layers[::-1] + [n_items]\n",
    "n_epochs = 50\n",
    "k = 5\n",
    "dropout = 0.3\n",
    "\n",
    "params = {}\n",
    "\n",
    "model = TrainableMultVAE(encoder_dims, decoder_dims, dropout=dropout)\n",
    "\n",
    "optimizer_kwargs = {\"weight_decay\":0, \"lr\":5e-4}\n",
    "# weigth decay==0 means not used\n",
    "optimizer = optim.Adam(model.parameters(), **optimizer_kwargs)\n",
    "criterion = loss_function\n",
    "\n",
    "# preparing validation data\n",
    "val_data = [(us_to_ids[u], gs_to_ids[i]) for u,i in validation_list]\n",
    "\n",
    "beta = 0.4\n",
    "\n",
    "model.fit(train_data, optimizer, criterion, val_data=val_data, n_epochs=n_epochs, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb51c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMultiVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layers, decoder_layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        if encoder_layers[-1] != decoder_layers[0]:\n",
    "            raise ValueError('Output from Encoder must have the same dimension as input to the Decoder.')\n",
    "        \n",
    "        \n",
    "        # last layer of encoder is for both mean and variance\n",
    "        encoder_layers[-1] = encoder_layers[-1]*2\n",
    "        \n",
    "        self.encoder_dims = encoder_layers\n",
    "        self.decoder_dims = decoder_layers\n",
    "        self.latent_dim = decoder_layers[0]\n",
    "        \n",
    "        self.encoder_layers = self.initialize_layers(encoder_layers)\n",
    "        self.decoder_layers = self.initialize_layers(decoder_layers)\n",
    "\n",
    "        \n",
    "        self.dropout_ = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def initialize_layers(self, layers, nonlinearity=\"tanh\"):\n",
    "        res = []\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            layer = nn.Linear(layers[i], layers[i+1])\n",
    "            self.init_weights(layer)\n",
    "            res.append(layer)\n",
    "            \n",
    "            if i != len(layers)-2:\n",
    "                res.append(nn.Tanh())\n",
    "\n",
    "        return nn.Sequential(*res)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_):\n",
    "        mu, logvar = self.encode(input_)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def encode(self, input_):\n",
    "        x = F.normalize(input_)\n",
    "        x = self.dropout_(x)\n",
    "        \n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            x = layer(x)\n",
    "            if i == len(self.encoder_layers) - 1:\n",
    "                mu = x[:, :self.encoder_dims[-1]//2]\n",
    "                logvar = x[:, self.encoder_dims[-1]//2:]\n",
    "                \n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # when in training mode we sample from \n",
    "        # normal distribution\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            # if not in training mode we want to\n",
    "            # get the mu as reparametrization\n",
    "            return mu\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = z\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        # Xavier Initialization for weights\n",
    "        size = layer.weight.size()\n",
    "        fan_out = size[0]\n",
    "        fan_in = size[1]\n",
    "        std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "        layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "        # Normal Initialization for Biases\n",
    "        layer.bias.data.normal_(0.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f63406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableMultVAE(BaseMultiVAE):\n",
    "    \n",
    "    def __init__(self, encoder_layers, decoder_layers):\n",
    "        super().__init__(encoder_layers, decoder_layers)\n",
    "        \n",
    "        \n",
    "    def fit(self, train_data, optimizer, criterion, n_epochs=100, batch_size=256, total_anneal_steps=200000,\n",
    "              anneal_cap=0.2, log_interval=100, k=10, val_data=None):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation = True if val_data is not None else False\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.k = k\n",
    "        self.total_anneal_steps = total_anneal_steps\n",
    "        self.anneal_cap = anneal_cap\n",
    "        self.log_interval = log_interval\n",
    "        self.n_users, self.n_items = train_data.shape\n",
    "        self.NDCGs = []\n",
    "        self.ERRs = []\n",
    "        self.HRs = []\n",
    "        self.training_time_seconds = []\n",
    "        self.validating_time_seconds = []\n",
    "        \n",
    "        idxlist = list(range(self.n_users))\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            np.random.shuffle(idxlist)\n",
    "            \n",
    "            epoch_start_time = time.time()\n",
    "            print(\"Training phase...\")\n",
    "            self.train_one_epoch(train_data, idxlist, epoch_num=epoch+1)\n",
    "            print(f\"Training took {round(time.time() - epoch_start_time,2)} seconds.\")\n",
    "            self.training_time_seconds.append(round(time.time() - epoch_start_time,2))\n",
    "            \n",
    "            if self.validation:\n",
    "                print(\"Evaluation phase...\")\n",
    "                val_epoch_start_time = time.time()\n",
    "                ndcg, err, hr = self.validate(train_data, val_data)\n",
    "                print(f\"Validating took {round(time.time() - val_epoch_start_time,2)} seconds.\")\n",
    "                self.validating_time_seconds.append(round(time.time() - val_epoch_start_time,2))\n",
    "            \n",
    "            print('-' * 89)\n",
    "            print('| end of epoch {:3d} | time: {:4.2f}s | '\n",
    "                    'NDCG@10 {:5.3f} | ERR@10 {:5.3f} | HR@10 {:5.3f}'.format(\n",
    "                        epoch+1, time.time() - epoch_start_time, ndcg, err, hr))\n",
    "            print('-' * 89)\n",
    "            \n",
    "\n",
    "    def train_one_epoch(self, train_data, idxlist, epoch_num):\n",
    "        \n",
    "        self.train()\n",
    "        train_loss = 0.0\n",
    "        update_count = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, start_idx in enumerate(range(0, self.n_users, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, self.n_users)\n",
    "            \n",
    "            # selecting appriopriate chunk of data and conveting \n",
    "            # it to tensors\n",
    "            data = train_data[idxlist[start_idx:end_idx]]\n",
    "            data = naive_sparse2tensor(data)\n",
    "            \n",
    "            # annealing\n",
    "            if self.total_anneal_steps > 0:\n",
    "                anneal = min(self.anneal_cap, \n",
    "                                1. * update_count / self.total_anneal_steps)\n",
    "            else:\n",
    "                anneal = self.anneal_cap\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = self.forward(data)\n",
    "\n",
    "            loss = self.criterion(recon_batch, data, mu, logvar, anneal)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            update_count += 1\n",
    "            \n",
    "            if batch_idx % self.log_interval == 0 and batch_idx > 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
    "                        'loss {:4.2f}'.format(\n",
    "                            epoch_num, batch_idx, len(range(0, self.n_users, self.batch_size)),\n",
    "                            elapsed * 1000 / self.log_interval,\n",
    "                            train_loss / self.log_interval))\n",
    "            \n",
    "                start_time = time.time()\n",
    "                train_loss = 0.0\n",
    "            \n",
    "    \n",
    "    def validate(self, train_data, val_data):\n",
    "        \"\"\"\n",
    "        val_data - list with n_users tuples of (user_id, item_id) \n",
    "        \"\"\"\n",
    "        \n",
    "        # Evaluation phase\n",
    "        self.eval()\n",
    "        val_res = {}\n",
    "        unique_users = list(range(self.n_users))\n",
    "        with torch.no_grad():\n",
    "            for start_idx in range(0, self.n_users, self.batch_size):\n",
    "                end_idx = min(start_idx+self.batch_size, self.n_users)\n",
    "                \n",
    "                data = train_data[start_idx:end_idx]\n",
    "                data_tensor = naive_sparse2tensor(data)\n",
    "                \n",
    "                # predict\n",
    "                pred, mu, logvar = self.forward(data_tensor)\n",
    "                # exclude examples from train set\n",
    "                pred[data.nonzero()] = -float(\"Inf\")\n",
    "                _, rec = torch.topk(pred, self.k, dim=-1)\n",
    "                \n",
    "                # append the results\n",
    "                uid = start_idx\n",
    "                for u_rec in rec.numpy():\n",
    "                    val_res[uid] = u_rec\n",
    "                    uid+=1\n",
    "        \n",
    "        # evaluation \n",
    "        ev = Evaluator(k=self.k, true=val_data, predicted=val_res)\n",
    "        ev.calculate_metrics()\n",
    "        ndcg, err, hr = ev.ndcg, ev.err, ev.hr\n",
    "        self.NDCGs.append(ndcg)\n",
    "        self.ERRs.append(err)\n",
    "        self.HRs.append(hr)\n",
    "        return ndcg, err, hr\n",
    "    \n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def save_model_params(self, path):\n",
    "        with open(path, \"wb\") as handle:\n",
    "            pickle.dump(self.__dict__, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dims = [n_items, 600, 200]\n",
    "decoder_dims = [200, 600, n_items]\n",
    "model = BaseMultiVAE(encoder_dims, decoder_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3726b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cb7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dims = [n_items, 600, 200]\n",
    "decoder_dims = [200, 600, n_items]\n",
    "\n",
    "model = TrainableMultVAE(encoder_dims, decoder_dims)\n",
    "\n",
    "optimizer_kwargs = {\"weight_decay\":0, \"lr\":5e-4}\n",
    "# weigth decay==0 means not used\n",
    "optimizer = optim.Adam(model.parameters(), **optimizer_kwargs)\n",
    "criterion = loss_function\n",
    "\n",
    "# preparing validation data\n",
    "val_data = [(us_to_ids[u], gs_to_ids[i]) for u,i in validation_list]\n",
    "\n",
    "\n",
    "model.fit(train_data, optimizer, criterion, val_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbaaa3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78836c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = torch.load(savepath+\"vae1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2 = TrainableMultVAE(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb0487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434a5722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1bb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558546d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
